permissions:
  - '*'
sessions:
  session:
    redirect: false
    openAI: true


jobs:
  preprocessing:
      working-directory: ${{ inputs.service.rundir }}
      ssh:
        remoteHost: ${{ inputs.cluster.resource.ip }}
      steps:
        - name: Checkout
          uses: parallelworks/checkout
          with:
            repo: https://github.com/parallelworks/activate-rag-vllm.git
            branch: nemotron-aecm
        - name: Create Environment File
          early-cancel: any-job-failed
          run: |
            # FIXME: remove when issue 11915 is fixed
            rm -f SESSION_PORT job.started job.ended run.out HOSTNAME
            rm -rf logs
            cp -rfT ${PW_PARENT_JOB_DIR}/ ./
            cat > .run.env << 'EOF'
            export RUNMODE="${{ inputs.service.runmode }}"
            export BUILD="${{ inputs.service.build }}"
            export RUNTYPE="${{ inputs.service.runtype }}"
            export SYSTEM_PROMPT="${{ inputs.service.systemprompt }}"
            export HF_TOKEN="${{ inputs.service.hftoken }}"
            export MODEL_NAME="${{ inputs.service.hfmodel }}"
            export API_KEY="${{ inputs.service.apikey }}"
            export DOCS_DIR="${{ inputs.service.docsdir }}"
            export VLLM_EXTRA_ARGS="${{ inputs.service.vllm_extra_args }}"
            export TRANSFORMERS_OFFLINE=1
            export TIKTOKEN_ENCODINGS_BASE="/root/.cache/tiktoken_encodings"
            export VLLM_ATTENTION_BACKEND="${{ inputs.service.advanced_settings.vllm_attention_backend }}"
            EOF
        - name: Controller Preprocessing
          run: |
            set -x
            export service_runmode=${{ inputs.service.runmode }}
            bash controller.sh
        - name: Create Service Script
          run: |
            set -x
            # Write code common to all services
            cat > start_service_mod.sh << 'EOF'

            if [ -z "${service_port}" ]; then
              service_port=$(pw agent open-port)
            fi

            if [ -z "${service_port}" ]; then
              echo "$(date) ERROR: No service port found"
              exit 1            
            fi
            echo ${service_port} > SESSION_PORT
            hostname > HOSTNAME

            cleanup() {
                echo "$(date) Cleaning up..."
                kill -- -$$
            }

            trap cleanup EXIT INT TERM

            echo
            echo
            echo "$(date) STARTING SERVICE"
            echo
            touch job.started
            EOF
            pwd
            cat start_service_mod.sh
            cat start_service.sh >> start_service_mod.sh

  session_runner:
    working-directory: ${{ inputs.service.rundir }}
    needs:
      - preprocessing
    ssh:
        remoteHost: ${{ inputs.cluster.resource.ip }}
    steps:
      - uses: marketplace/script_submitter/v3.5
        early-cancel: any-job-failed
        with:
          resource: ${{ inputs.cluster.resource }}
          shebang: '#!/bin/bash'
          rundir: ${{ inputs.service.rundir }}
          use_existing_script: true
          script_path: "${{ inputs.service.rundir }}/start_service_mod.sh"
          scheduler: ${{ inputs.cluster.scheduler }}
          slurm:
            is_disabled: ${{ inputs.cluster.slurm.is_disabled }}
            slurm_options: ${{ inputs.cluster.slurm.slurm_options }}
            partition_default: ${{ inputs.cluster.slurm.partition_default }}
            partition_hpc4: ${{ inputs.cluster.slurm.partition_hpc4
            cpus_per_task: ${{ inputs.cluster.slurm.cpus_per_task }}
            mem: ${{ inputs.cluster.slurm.mem }}
            gres_gpu_default: ${{ inputs.cluster.slurm.gres_gpu_default }}
            gres_gpu_hpc4: ${{ inputs.cluster.slurm.gres_gpu_hpc4 }}
            time: ${{ inputs.cluster.slurm.time }}
            scheduler_directives: ${{ inputs.cluster.slurm.scheduler_directives }}
          pbs:
            is_disabled: ${{ inputs.cluster.pbs.is_disabled }}
            scheduler_directives: ${{ inputs.cluster.pbs.scheduler_directives }}
      - name: Notify job ended
        early-cancel: any-job-failed
        run: |
          set -x
          pwd
          touch job.ended
          ls -lat job.ended


  wait_for_job_start:
    working-directory: ${{ inputs.service.rundir }}
    needs:
      - preprocessing
    ssh:
      remoteHost: ${{ inputs.cluster.resource.ip }}
    steps:
      - name: Wait for job to start
        early-cancel: any-job-failed
        run: |
          set -x
          while [ ! -f job.started ]; do
            if [ -f job.ended ]; then
              echo "$(date) ERROR: Job ended before it started. Exiting."
              exit 1
            fi
            echo "$(date) Waiting for job to start..."
            sleep 5
          done
      - name: Get Hostname
        early-cancel: any-job-failed
        run: |
          set -x
          HOSTNAME=$(cat HOSTNAME | cut -d'.' -f1)
          echo "HOSTNAME=${HOSTNAME}" | tee -a $OUTPUTS

          if [ -z "${HOSTNAME}" ]; then
            echo "$(date) Failed to get target hostname"
            exit 1
          fi
          sleep 5

  cleanup:
    working-directory: ${{ inputs.service.rundir }}
    if: ${{ always }}
    needs:
      - session_runner
      - wait_for_job_start
    ssh:
      remoteHost: ${{ inputs.cluster.resource.ip }}
    steps:
      - name: Controller cleanup
        if: ${{ inputs.cluster.slurm.is_disabled && inputs.cluster.pbs.is_disabled }}
        run: echo "$(date) Cleaning up..."
        cleanup: |
          set -x
          if [ -f cancel.sh ]; then
            bash cancel.sh
          fi
      - name: Compute cleanup
        if: ${{ (inputs.cluster.slurm.is_disabled == false || inputs.cluster.pbs.is_disabled == false) }}
        run: echo "Cleaning up..."
        cleanup: |
          set -x
          remote_host="${{ needs.wait_for_job_start.outputs.HOSTNAME }}"
          if [ -z "${remote_host}" ]; then
            echo "$(date) WARNING: Compute node's hostname is missing. Exiting step."
            exit 0
          fi
          sshcmd="ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=5 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${remote_host}"
          if [ -f cancel.sh ]; then
            ${sshcmd} 'bash -s' < ${PWD}/cancel.sh
          fi

  create_session:
    working-directory: ${{ inputs.service.rundir }}
    needs:
      - wait_for_job_start
    ssh:
      remoteHost: ${{ inputs.cluster.resource.ip }}
    steps:
      - name: Get Session Port
        early-cancel: any-job-failed
        run: |
          set -x
          SESSION_PORT=$(cat SESSION_PORT)
          echo "SESSION_PORT=${SESSION_PORT}" | tee -a $OUTPUTS

          if [ -z "${SESSION_PORT}" ]; then
            echo "$(date) Failed to get target session's port"
            exit 1
          fi
          sleep 5
      - name: Wait for Server To Start
        early-cancel: any-job-failed
        run: |
          TIMEOUT=5
          RETRY_INTERVAL=3
          remote_host="${{ needs.wait_for_job_start.outputs.HOSTNAME }}"
          remote_port="${{ needs.create_session.outputs.SESSION_PORT }}"

          # Function to check if server is listening
          check_server() {
              curl --silent --connect-timeout "$TIMEOUT" "http://${remote_host}:${remote_port}" >/dev/null 2>&1
              return $?
          }

          # Main loop
          attempt=1
          while true; do
              echo "$(date) Attempt $attempt: Checking if server is listening on ${remote_host}:${remote_port}..."
              
              if check_server; then
                  echo "$(date) Success: Server is listening on ${remote_host}:${remote_port}!"
                  sleep 40
                  exit 0
              elif [ -f job.ended ]; then
                  echo "$(date) Job was completed. Exiting... "
                  exit 0
              else
                  echo "$(date) Server not responding. Retrying in ${RETRY_INTERVAL} seconds..."
                  sleep "$RETRY_INTERVAL"
                  ((attempt++))
              fi
          done
          sleep 5

      - name: Update Session
        uses: parallelworks/update-session
        with:
          target: '${{ inputs.cluster.resource.id }}'
          name: '${{ sessions.session }}'
          remoteHost: '${{ needs.wait_for_job_start.outputs.HOSTNAME }}'
          remotePort: '${{ needs.create_session.outputs.SESSION_PORT }}'
          localPort: '${{ inputs.service.localport }}' # Chosen at random if empty


'on':
  execute:
    inputs:
      cluster:
        type: group
        label: Compute Cluster Settings
        items:
          resource:
            type: compute-clusters
            label: Service host
            include-workspace: false
            tooltip: Resource to host the service
            autoselect: true
          scheduler:
            type: boolean
            default: true
            label: Schedule Job?
            hidden: true
            tooltip: |
              Yes → Job is submitted to the scheduler using sbatch, qsub, etc
              No  → Job is executed in the controller or login node instead
          slurm:
            type: group
            label: SLURM Directives
            hidden: ${{ inputs.cluster.resource.provider == 'existing' && inputs.cluster.resource.schedulerType != 'slurm' || inputs.scheduler == false }}
            ignore: ${{ inputs.cluster.resource.provider == 'existing' && inputs.cluster.resource.schedulerType != 'slurm' || inputs.scheduler == false }}
            items:
              slurm_options:
                type: dropdown
                label: Select Cluster
                optional: true
                default: ''
                options:
                  - value: ''
                    label: Default
                  - value: '-M hpc4'
                    label: HPC4
              is_disabled:
                type: boolean
                hidden: true
                default: ${{ inputs.cluster.resource.provider == 'existing' && inputs.cluster.resource.schedulerType != 'slurm' || inputs.scheduler == false }}
                label: Is SLURM disabled?
              partition_default:
                type: slurm-partitions
                label: SLURM partition
                ignore: ${{ '-M hpc4' == inputs.cluster.slurm.slurm_options }}
                hidden: ${{ .ignore }}
                optional: true
                resource: ${{ inputs.cluster.resource }}
                tooltip: Select a partition from the drop down menu. Leave empty to let SLURM pick a partition.
              partition_hpc4:
                type: dropdown
                label: SLURM partition
                optional: true
                tooltip: Select a partition from the drop down menu. Leave empty to let SLURM pick a partition.
                ignore: ${{ '-M hpc4' != inputs.cluster.slurm.slurm_options }}
                hidden: ${{ .ignore }}
                default: normal
                options:
                  - normal
                  - gpu
                  - gpu-h200
                  - gpu-quick
                  - ht
                  - large-mem
                  - quick
                  - test
                  - unlimited
              cpus_per_task:
                type: number
                label: CPUs per task
                min: 1
                max: 32
                default: 1
                tooltip: '--cpus-per-task=value slurm directive'
                ignore: ${{ 'existing' != inputs.cluster.resource.provider }}
                hidden: ${{ .ignore }}
              mem:
                type: string
                label: Minimum total memory required
                default: 8GB
                tooltip: '--mem=value slurm directive'
                hidden: ${{ 'existing' != inputs.cluster.resource.provider }}
                ignore: ${{ .hidden }}
                optional: true
              gres_gpu_default:
                type: number
                label: Number of GPUs
                ignore: ${{ ( inputs.cluster.slurm.partition_default != 'gpu' && inputs.cluster.slurm.partition_default != 'gpu-quick' ) || 'existing' != inputs.cluster.resource.provider  }}
                hidden: ${{ .ignore }}
                min: 1
                max: 4
                default: 1
                tooltip: '--gres=gpu:X slurm directive'
              gres_gpu_hpc4:
                type: number
                label: Number of GPUs
                hidden: ${{ ( inputs.cluster.slurm.partition_hpc4 != 'gpu' && inputs.cluster.slurm.partition_hpc4 != 'gpu-quick' && inputs.cluster.slurm.partition_hpc4 != 'gpu-h200' ) || 'existing' != inputs.cluster.resource.provider  }}
                ignore: ${{ .hidden }}
                optional: ${{ .hidden }}
                min: 1
                max: 4
                default: 1
                tooltip: '--gres=gpu:X slurm directive'
              time:
                label: Walltime
                type: string
                default: '01:00:00'
                tooltip: '--time= SLURM directive to set the maximum wall-clock time limit for the job'
              scheduler_directives:
                type: editor
                optional: true
                tooltip: |
                  Type in additional scheduler directives. 
          pbs:
            type: group
            label: PBS Directives
            hidden: ${{ inputs.cluster.resource.schedulerType != 'pbs' || inputs.scheduler == false }}
            ignore: ${{ inputs.cluster.resource.schedulerType != 'pbs' || inputs.scheduler == false }}
            items:
              is_disabled:
                type: boolean
                hidden: true
                default: ${{ inputs.cluster.resource.schedulerType != 'pbs' || inputs.scheduler == false }}
                label: Is PBS disabled?
              scheduler_directives:
                label: Scheduler Directives
                type: editor
                tooltip: Type the PBS scheduler directives
      service:
        type: group
        label: Service
        items:
          runmode:
            label: Execution Mode
            type: dropdown
            default: singularity
            hidden: ${{ 'existing' == inputs.resource.provider }}
            options:
              - value: docker
                label: Docker
              - value: singularity
                label: Singularity
          rundir:
            label: Run Directory
            default: ${HOME}/pw/activate-rag-vllm2
            type: string
          runtype:
            label: Run Type
            type: dropdown
            options:
              - value: vllm
                label: vLLM Only
              - value: all
                label: vLLM+RAG
          build:
            label: Build Containers
            type: boolean
            default: false
          pull:
            label: Pull Containers
            hidden: ${{ inputs.service.runmode == 'docker' }}
            type: boolean
            default: true
          hfmodel:
            label: HF Model
            default: /p/work1/${USER}/Llama-3_3-Nemotron-Super-49B-v1_5
            type: string
          vllm_extra_args:
            label: VLLM Extra Args
            default: "--dtype bfloat16 --trust_remote_code --tensor-parallel-size 4 --async-scheduling --gpu-memory-utilization 0.85"
            placeholder: "--dtype bfloat16 --trust_remote_code --tensor-parallel-size 4 --async-scheduling --gpu-memory-utilization 0.85"
            type: string
          hftoken:
            label: HF Token (gated models)
            optional: true
            default: ${{ org.HF_TOKEN }}
            type: password
          apikey:
            label: vLLM API Key
            optional: true
            tooltip: Required for integration with Cline and other code assist tools.
            type: password
          docsdir:
            label: RAG Directory
            hidden: ${{ inputs.service.runtype != 'all' }}
            optional: true
            default: ./docs
            type: string
          systemprompt:
            type: string
            label: System Prompt
            hidden: ${{ inputs.service.runtype != 'all' }}
            textarea: true
            optional: true
            default: You are a careful assistant. Use ONLY the provided context blocks to answer. Each block is numbered [1], [2], … and includes source metadata. When you use information from a block, you MUST cite it inline with [n]. At the end of your response, include a 'References' section with one reference per line formatted as [n] file_path (chunk index). Do not invent citations or sources. If the context does not contain the answer, say so briefly.
          localport:
            label: User Workspace Port
            default: '5555'
            tooltip: Port that runs within the user workspace and used to connect to the code assist and chat interfaces.
            type: string

          advanced_settings:
            type: group
            label: Advanced Settings
            collapsed: true
            items:
              repository:
                type: string
                label: Repository
                default: https://github.com/parallelworks/activate-rag-vllm.git
              repository_branch:
                type: string
                label: Repository Branch
                default: nemotron
              tiktoken_encodings:
                label: Pull Encodings
                tooltip: For GPT-OSS pull the tiktoken encodings.
                type: boolean
                default: false
              vllm_attention_backend:
                type: dropdown
                label: VLLM Attention Backend
                default: TRITON_ATTN
                tooltip: Select the attention backend implementation used by vLLM
                options:
                  - value: FLASH_ATTN
                  - value: TRITON_ATTN
                  - value: ROCM_ATTN
                  - value: ROCM_AITER_MLA
                  - value: ROCM_AITER_TRITON_MLA
                  - value: ROCM_AITER_FA
                  - value: ROCM_AITER_MLA_SPARSE
                  - value: TORCH_SDPA
                  - value: FLASHINFER
                  - value: FLASHINFER_MLA
                  - value: TRITON_MLA
                  - value: CUTLASS_MLA
                  - value: FLASHMLA
                  - value: FLASHMLA_SPARSE
                  - value: FLASH_ATTN_MLA
                  - value: PALLAS
                  - value: IPEX
                  - value: NO_ATTENTION
                  - value: FLEX_ATTENTION
                  - value: TREE_ATTN
                  - value: ROCM_AITER_UNIFIED_ATTN
                  - value: CPU_ATTN
                  - value: CUSTOM