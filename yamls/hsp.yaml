permissions:
  - '*'
sessions:
  session:
    redirect: false
    openAI: true

jobs:
  prepare_job_directory:
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    steps:
      - name: Preparing Run Directory
        run: |
          mkdir -p $(dirname ${{ inputs.rundir }})
          git clone -b ${{ inputs.advanced_settings.repository_branch }} ${{ inputs.advanced_settings.repository }} ${{ inputs.rundir }}
          cd ${{ inputs.rundir }}
          git checkout ${{ inputs.advanced_settings.repository_branch }}
          git branch --set-upstream-to=origin/${{ inputs.advanced_settings.repository_branch }}
          git pull
          rm -f SESSION_PORT job.started job.ended run.out HOSTNAME
          rm -rf logs
      - name: Create Environment File
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ inputs.rundir }}
          echo "export RUNMODE=${{ inputs.runmode  }}" > .run.env
          echo "export BUILD=${{ inputs.build  }}" >> .run.env
          echo "export RUNTYPE=${{ inputs.runtype  }}" >> .run.env
          echo "export SYSTEM_PROMPT=\"${{ inputs.systemprompt }}\"" >> .run.env
          echo "export HF_TOKEN=${{ inputs.hftoken  }}" >> .run.env
          echo "export MODEL_NAME=${{ inputs.hfmodel  }}" >> .run.env
          echo "export API_KEY=${{ inputs.apikey  }}" >> .run.env
          echo "export DOCS_DIR=${{ inputs.docsdir }}" >> .run.env
          echo "export VLLM_EXTRA_ARGS=\"${{ inputs.vllm_extra_args }}\"" >> .run.env
          echo "export TRANSFORMERS_OFFLINE=1" >> .run.env
          echo "export TIKTOKEN_ENCODINGS_BASE=/root/.cache/tiktoken_encodings" >> .run.env
          echo "export VLLM_ATTENTION_BACKEND=${{ inputs.advanced_settings.vllm_attention_backend }}" >> .run.env

      - name: Install Singularity Compose
        if: ${{ inputs.runmode == 'singularity' }}
        early-cancel: any-job-failed
        run: |
          set -x
          # Check if singularity-compose is installed globally
          if ! command -v singularity-compose &> /dev/null; then
              # Check if virtual environment exists and activate it
              if [ -d ~/pw/software/singularity-compose ]; then
                  source ~/pw/software/singularity-compose/bin/activate
              fi
              # Check again if singularity-compose is available after activation
              if ! command -v singularity-compose &> /dev/null; then
                  echo "$(date) singularity-compose not found, installing..."
                  
                  # Create directory for Python environment
                  mkdir -p ~/pw/software
                  
                  # Create virtual environment named singularity-compose and install singularity-compose
                  python3 -m venv ~/pw/software/singularity-compose
                  source ~/pw/software/singularity-compose/bin/activate
                  pip install --upgrade pip
                  pip install singularity-compose
              fi
          fi
          if ! command -v singularity-compose >/dev/null 2>&1; then
            echo "$(date) ERROR: Failed to install singularity-compose"
            exit 1
          fi
      - name: Pull Singularity Containers
        if: ${{ inputs.runmode == 'singularity' && inputs.pull == true }}
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ inputs.rundir }}
          
          # vllm container
          if [[ ! -f "vllm.sif" ]]; then
            echo "vllm.sif not found, pulling from ${{ inputs.container_bucket }}"
            pw bucket cp "${{ inputs.container_bucket }}/vllm.sif" ./
          else
            echo "vllm.sif already exists, skipping pull"
          fi

          # rag container (only for runmode=all)
          if [[ "${{ inputs.runmode }}" == "all" ]]; then
            if [[ ! -f "rag.sif" ]]; then
              echo "rag.sif not found, pulling from ${{ inputs.container_bucket }}"
              pw bucket cp "${{ inputs.container_bucket }}/rag.sif" ./
            else
              echo "rag.sif already exists, skipping pull"
            fi
          fi

          echo Singularity container pull step complete.
      - name: Pull Tiktoken Encodings
        if: ${{ inputs.advanced_settings.tiktoken_encodings == true }}
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ inputs.rundir }}
          mkdir -p cache/tiktoken_encodings
          wget -O cache/tiktoken_encodings/o200k_base.tiktoken "https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken"
          wget -O cache/tiktoken_encodings/cl100k_base.tiktoken "https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken"


  vllm_runner:
    needs:
      - prepare_job_directory
    steps:
      - uses: marketplace/script_submitter/v3.3
        with:
          resource: ${{ inputs.resource }}
          shebang: '#!/bin/bash'
          rundir: ${{ inputs.rundir }}
          use_existing_script: true
          script_path: "${{ inputs.rundir }}/start_template.sh"
          scheduler: ${{ inputs.scheduler }}
          slurm:
            is_disabled: ${{ inputs.slurm.is_disabled }}
            account: ${{ inputs.slurm.account }}
            partition: ${{ inputs.slurm.partition }}
            qos: ${{ inputs.slurm.qos }}
            number_of_nodes: ${{ inputs.slurm.number_of_nodes }}
            cpus_per_task: ${{ inputs.slurm.cpus_per_task }}
            scheduler_directives: ${{ inputs.slurm.scheduler_directives }}
            time: ${{ inputs.slurm.time }}
          pbs:
            is_disabled: ${{ inputs.pbs.is_disabled }}
            account: ${{ inputs.pbs.account }}
            scheduler_directives: ${{ inputs.pbs.scheduler_directives }}

  create_session:
    needs:
      - prepare_job_directory
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    steps:
      - name: Wait for job to start
        early-cancel: any-job-failed
        run: |
          set -x
          while [ ! -f ${{ inputs.rundir }}/job.started ]; do
            echo "Waiting for job to start..."
            sleep 5
          done
      - name: Get Hostname
        if: ${{ inputs.pbs.is_disabled == false }}
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ inputs.rundir }}
          target_hostname=$(cat HOSTNAME)
          echo "target_hostname=${target_hostname}" | tee -a $OUTPUTS

          if [ -z "${target_hostname}" ]; then
            echo "$(date) Failed to get target hostname"
            exit 1
          fi
      - name: Get Session Port
        early-cancel: any-job-failed
        run: |
          set -euo pipefail
          set -x

          TIMEOUT=5
          RETRY_INTERVAL=3
          cd ${{ inputs.rundir }}

          attempt=1
          while true; do
              echo "$(date) Attempt $attempt: Checking for SESSION_PORT file..."

              if [ -f SESSION_PORT ]; then
                  echo "$(date) Success: SESSION_PORT file found!"
                  cat SESSION_PORT | tee -a "$OUTPUTS"
                  exit 0
              elif [ -f job.ended ]; then
                  echo "$(date) Job was completed but SESSION_PORT was never created. Exiting..."
                  exit 1
              else
                  echo "$(date) SESSION_PORT not found. Retrying in ${RETRY_INTERVAL} seconds..."
                  sleep "$RETRY_INTERVAL"
                  ((attempt++))
              fi
          done
      - name: Wait for Server To Start
        early-cancel: any-job-failed
        run: |
          TIMEOUT=5
          RETRY_INTERVAL=3
          remote_host="${{ needs.create_session.outputs.target_hostname }}"
          remote_port="${{ needs.create_session.outputs.SESSION_PORT }}"

          # Function to check if server is listening
          check_server() {
              curl --silent --connect-timeout "$TIMEOUT" "http://${remote_host}:${remote_port}" >/dev/null 2>&1
              return $?
          }

          cd ${{ inputs.rundir }}

          # Main loop
          attempt=1
          while true; do
              echo "$(date) Attempt $attempt: Checking if server is listening on ${remote_host}:${remote_port}..."
              
              if check_server; then
                  echo "$(date) Success: Server is listening on ${remote_host}:${remote_port}!"
                  exit 0
              elif [ -f job.ended ]; then
                  echo "$(date) Job was completed. Exiting... "
                  exit 0
              else
                  echo "$(date) Server not responding. Retrying in ${RETRY_INTERVAL} seconds..."
                  sleep "$RETRY_INTERVAL"
                  ((attempt++))
              fi
          done
      - name: Update Session
        uses: parallelworks/update-session
        with:
          remotePort: '${{ needs.create_session.outputs.SESSION_PORT }}'
          target: '${{ inputs.resource.id }}'
          name: '${{ sessions.session }}'
          remoteHost: '${{ needs.create_session.outputs.target_hostname }}'
          localPort: '${{ inputs.localport }}'


'on':
  execute:
    inputs:
      resource:
        type: compute-clusters
        label: Compute Cluster
        autoselect: true
        include-workspace: false
        tooltip: Resource to run the service
      scheduler:
        type: boolean
        default: false
        label: Schedule Job?
        hidden: ${{ inputs.resource.schedulerType == '' }}
        ignore: ${{ .hidden }}
        tooltip: |
          Yes → Job is submitted to the scheduler using sbatch, qsub, etc
          No  → Job is executed in the controller or login node instead
      runmode:
        label: Execution Mode
        type: dropdown
        default: singularity
        hidden: ${{ 'existing' == inputs.resource.provider }}
        options:
          - value: docker
            label: Docker
          - value: singularity
            label: Singularity
      rundir:
        label: Run Directory
        default: ~/pw/activate-rag-vllm
        type: string
      runtype:
        label: Run Type
        type: dropdown
        options:
          - value: vllm
            label: vLLM Only
          - value: all
            label: vLLM+RAG
      build:
        label: Build Containers
        type: boolean
        default: false
      pull:
        label: Pull Containers
        hidden: ${{ inputs.runmode == 'docker' }}
        type: boolean
        default: true
      container_bucket:
        label: Container Bucket (.sif)
        hidden: ${{ inputs.runmode != 'singularity' || inputs.pull != true }}
        type: string
        default: pw://mshaxted/codeassist
        tooltip: PW singularity container bucket that holds vllm.sif and rag.sif containers
      hfmodel:
        label: HF Model
        default: /p/work1/${USER}/Llama-3_3-Nemotron-Super-49B-v1_5
        type: string
      vllm_extra_args:
        label: VLLM Extra Args
        default: "--dtype bfloat16 --trust_remote_code --tensor-parallel-size 4 --async-scheduling --gpu-memory-utilization 0.85"
        placeholder: "--dtype bfloat16 --trust_remote_code --tensor-parallel-size 4 --async-scheduling --gpu-memory-utilization 0.85"
        type: string
      hftoken:
        label: HF Token (gated models)
        optional: true
        default: ${{ org.HF_TOKEN }}
        type: password
      apikey:
        label: vLLM API Key
        optional: true
        tooltip: Required for integration with Cline and other code assist tools.
        type: password
      docsdir:
        label: RAG Directory
        hidden: ${{ inputs.runtype != 'all' }}
        optional: true
        default: ./docs
        type: string
      systemprompt:
        type: string
        label: System Prompt
        hidden: ${{ inputs.runtype != 'all' }}
        textarea: true
        optional: true
        default: You are a careful assistant. Use ONLY the provided context blocks to answer. Each block is numbered [1], [2], … and includes source metadata. When you use information from a block, you MUST cite it inline with [n]. At the end of your response, include a 'References' section with one reference per line formatted as [n] file_path (chunk index). Do not invent citations or sources. If the context does not contain the answer, say so briefly.
      localport:
        label: User Workspace Port
        default: '5555'
        tooltip: Port that runs within the user workspace and used to connect to the code assist and chat interfaces.
        type: string
      slurm:
        type: group
        label: SLURM Directives
        hidden: ${{ inputs.resource.provider == 'existing' && inputs.resource.schedulerType != 'slurm' || inputs.scheduler == false }}
        items:
          is_disabled:
            type: boolean
            hidden: true
            default: ${{ inputs.resource.provider == 'existing' && inputs.resource.schedulerType != 'slurm' || inputs.scheduler == false }}
            label: Is SLURM disabled?
          account:
            label: SLURM account
            type: slurm-accounts
            resource: ${{ inputs.resource }}
            ignore: ${{ inputs.resource.provider == 'existing' && inputs.resource.schedulerType != 'slurm' || inputs.scheduler == false }}
            optional: ${{ .ignore }}
            tooltip: Select an account from the drop down menu.
          partition:
            type: slurm-partitions
            label: SLURM partition
            ignore: ${{ inputs.resource.provider == 'existing' && inputs.resource.schedulerType != 'slurm' || inputs.scheduler == false }}
            optional: true
            resource: ${{ inputs.resource }}
            tooltip: Select a partition from the drop down menu.
          qos:
            label: Quality of Service [QoS]
            type: slurm-qos
            resource: ${{ inputs.resource }}
            tooltip: Select a QOS from the drop down menu.
            ignore: ${{ inputs.resource.provider == 'existing' && inputs.resource.schedulerType != 'slurm' || inputs.scheduler == false }}
            optional: ${{ .ignore }}
          number_of_nodes:
            type: number
            label: Number of Nodes
            ignore: ${{ inputs.resource.provider == 'existing' && inputs.resource.schedulerType != 'slurm' || inputs.scheduler == false }}
            min: 1
            max: 50
            default: 1
            tooltip: '--nodes= SLURM directive to set the number of nodes'
          cpus_per_task:
            type: number
            label: CPUs per task
            ignore: ${{ inputs.resource.provider == 'existing' && inputs.resource.schedulerType != 'slurm' || inputs.scheduler == false }}
            min: 1
            max: 500
            default: 1
            tooltip: '--cpus-per-task= SLURM directive to set the number of CPUs per task'
          time:
            label: Walltime
            type: string
            default: '01:00:00'
            ignore: ${{ inputs.resource.provider == 'existing' && inputs.resource.schedulerType != 'slurm' || inputs.scheduler == false }}
            tooltip: '--time= SLURM directive to set the maximum wall-clock time limit for the job'
          scheduler_directives:
            type: editor
            ignore: ${{ inputs.resource.provider == 'existing' && inputs.resource.schedulerType != 'slurm' || inputs.scheduler == false }}
            optional: true
            tooltip: |
              Type in additional scheduler directives. 
      pbs:
        type: group
        label: PBS Directives
        hidden: ${{ inputs.resource.schedulerType != 'pbs' || inputs.scheduler == false }}
        items:
          is_disabled:
            type: boolean
            hidden: true
            default: ${{ inputs.resource.schedulerType != 'pbs' || inputs.scheduler == false }}
            label: Is PBS disabled?
          account:
            label: Account
            type: string
            ignore: ${{ inputs.resource.schedulerType != 'pbs' || inputs.scheduler == false }}
          scheduler_directives:
            label: Scheduler Directives
            type: editor
            tooltip: Type the PBS scheduler directives
            ignore: ${{ inputs.resource.schedulerType != 'pbs' || inputs.scheduler == false }}
