permissions:
  - '*'
sessions:
  session:
    redirect: false
    openAI: true

jobs:
  prepare_job_directory:
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    steps:
      - name: Preparing Run Directory
        run: |
          mkdir -p $(dirname ${{ inputs.advanced_settings.rundir }})
          git clone -b ${{ inputs.advanced_settings.repository_branch }} ${{ inputs.advanced_settings.repository }} ${{ inputs.advanced_settings.rundir }}
          cd ${{ inputs.advanced_settings.rundir }}
          git checkout ${{ inputs.advanced_settings.repository_branch }}
          git branch --set-upstream-to=origin/${{ inputs.advanced_settings.repository_branch }}
          git pull
          rm -f jobid SESSION_PORT job.started job.ended run.out HOSTNAME
          rm -rf logs
      
      - name: Create Environment File
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ inputs.advanced_settings.rundir }}
          
          # Core configuration
          echo "export RUNMODE=${{ inputs.runmode }}" > .run.env
          echo "export RUNTYPE=${{ inputs.runtype }}" >> .run.env
          echo "export SYSTEM_PROMPT=\"${{ inputs.rag.systemprompt }}\"" >> .run.env
          echo "export HF_TOKEN=${{ inputs.model.hf_token }}" >> .run.env
          echo "export API_KEY=${{ inputs.advanced_settings.apikey }}" >> .run.env
          echo "export DOCS_DIR=${{ inputs.rag.docsdir }}" >> .run.env
          
          # Build vLLM extra args from structured inputs (skip dtype if custom)
          if [[ "${{ inputs.vllm.dtype }}" == "custom" ]]; then
            VLLM_ARGS="--tensor-parallel-size ${{ inputs.vllm.num_gpus }} --gpu-memory-utilization ${{ inputs.vllm.gpu_memory }}"
          else
            VLLM_ARGS="--dtype ${{ inputs.vllm.dtype }} --tensor-parallel-size ${{ inputs.vllm.num_gpus }} --gpu-memory-utilization ${{ inputs.vllm.gpu_memory }}"
          fi
          if [[ "${{ inputs.vllm.max_model_len }}" != "auto" ]]; then
            VLLM_ARGS="$VLLM_ARGS --max-model-len ${{ inputs.vllm.max_model_len }}"
          fi
          if [[ -n "${{ inputs.vllm.extra_args }}" ]]; then
            VLLM_ARGS="$VLLM_ARGS ${{ inputs.vllm.extra_args }}"
          fi
          echo "export VLLM_EXTRA_ARGS=\"$VLLM_ARGS\"" >> .run.env
          
          # Model configuration
          echo "export MODEL_SOURCE=${{ inputs.model.source }}" >> .run.env
          if [[ "${{ inputs.model.source }}" == "local" ]]; then
            echo "export MODEL_NAME=${{ inputs.model.local_path }}" >> .run.env
            echo "export MODEL_PATH=${{ inputs.model.local_path }}" >> .run.env
            echo "export TRANSFORMERS_OFFLINE=1" >> .run.env
          else
            echo "export MODEL_NAME=${{ inputs.model.hf_model_id }}" >> .run.env
            echo "export HF_MODEL_ID=${{ inputs.model.hf_model_id }}" >> .run.env
            echo "export MODEL_CACHE_BASE=${{ inputs.model.cache_dir }}" >> .run.env
          fi
          
          # Container paths (if using existing containers)
          if [[ "${{ inputs.container.source }}" == "path" ]]; then
            echo "export VLLM_CONTAINER_PATH=${{ inputs.container.vllm_path }}" >> .run.env
            echo "export RAG_CONTAINER_PATH=${{ inputs.container.rag_path }}" >> .run.env
          fi
          
          # vLLM attention backend
          if [[ -n "${{ inputs.advanced_settings.vllm_attention_backend }}" ]]; then
            echo "export VLLM_ATTENTION_BACKEND=${{ inputs.advanced_settings.vllm_attention_backend }}" >> .run.env
          fi
          
          # Tiktoken encodings path
          if [[ "${{ inputs.advanced_settings.tiktoken_encodings }}" == "true" ]]; then
            echo "export TIKTOKEN_ENCODINGS_BASE=/root/.cache/tiktoken_encodings" >> .run.env
          fi

      - name: Install Apptainer Compose
        if: ${{ inputs.runmode == 'singularity' }}
        early-cancel: any-job-failed
        run: |
          set -x
          # Check if singularity-compose is installed globally
          if ! command -v singularity-compose &> /dev/null; then
              # Check if virtual environment exists and activate it
              if [ -d ~/pw/software/singularity-compose ]; then
                  source ~/pw/software/singularity-compose/bin/activate
              fi
              # Check again if singularity-compose is available after activation
              if ! command -v singularity-compose &> /dev/null; then
                  echo "$(date) singularity-compose not found, installing..."
                  
                  # Create directory for Python environment
                  mkdir -p ~/pw/software
                  
                  # Create virtual environment and install singularity-compose
                  python3 -m venv ~/pw/software/singularity-compose
                  source ~/pw/software/singularity-compose/bin/activate
                  pip install --upgrade pip
                  pip install singularity-compose
              fi
          fi
          if ! command -v singularity-compose >/dev/null 2>&1; then
            echo "$(date) ERROR: Failed to install singularity-compose"
            exit 1
          fi
      
      - name: Pull Apptainer Containers
        if: ${{ inputs.runmode == 'singularity' && inputs.container.source == 'pull' }}
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ inputs.advanced_settings.rundir }}
          
          # vllm container
          if [[ ! -f "vllm.sif" ]]; then
            echo "vllm.sif not found, pulling from ${{ inputs.container.bucket }}"
            pw bucket cp "${{ inputs.container.bucket }}/vllm.sif" ./
          else
            echo "vllm.sif already exists, skipping pull"
          fi

          # rag container (only for runtype=all)
          if [[ "${{ inputs.runtype }}" == "all" ]]; then
            if [[ ! -f "rag.sif" ]]; then
              echo "rag.sif not found, pulling from ${{ inputs.container.bucket }}"
              pw bucket cp "${{ inputs.container.bucket }}/rag.sif" ./
            else
              echo "rag.sif already exists, skipping pull"
            fi
          fi

          echo "Apptainer container pull step complete."
      
      - name: Link Existing Containers
        if: ${{ inputs.runmode == 'singularity' && inputs.container.source == 'path' }}
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ inputs.advanced_settings.rundir }}
          
          # Link vllm container
          if [[ -f "${{ inputs.container.vllm_path }}" ]]; then
            ln -sf "${{ inputs.container.vllm_path }}" ./vllm.sif
            echo "Linked vllm.sif from ${{ inputs.container.vllm_path }}"
          else
            echo "ERROR: vLLM container not found at ${{ inputs.container.vllm_path }}"
            exit 1
          fi
          
          # Link rag container (only for runtype=all)
          if [[ "${{ inputs.runtype }}" == "all" ]]; then
            if [[ -f "${{ inputs.container.rag_path }}" ]]; then
              ln -sf "${{ inputs.container.rag_path }}" ./rag.sif
              echo "Linked rag.sif from ${{ inputs.container.rag_path }}"
            else
              echo "ERROR: RAG container not found at ${{ inputs.container.rag_path }}"
              exit 1
            fi
          fi
      
      - name: Build Apptainer Containers
        if: ${{ inputs.runmode == 'singularity' && inputs.container.source == 'build' }}
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ inputs.advanced_settings.rundir }}
          
          echo "Building Apptainer containers from source..."
          echo "Note: This requires sudo or fakeroot capabilities"
          
          # Build vllm container
          if [[ ! -f "vllm.sif" ]]; then
            echo "Building vllm.sif..."
            if sudo -n true 2>/dev/null; then
              sudo singularity build vllm.sif singularity/Singularity.vllm
            else
              singularity build --fakeroot vllm.sif singularity/Singularity.vllm
            fi
          else
            echo "vllm.sif already exists, skipping build"
          fi
          
          # Build rag container (only for runtype=all)
          if [[ "${{ inputs.runtype }}" == "all" ]]; then
            if [[ ! -f "rag.sif" ]]; then
              echo "Building rag.sif..."
              if sudo -n true 2>/dev/null; then
                sudo singularity build rag.sif singularity/Singularity.rag
              else
                singularity build --fakeroot rag.sif singularity/Singularity.rag
              fi
            else
              echo "rag.sif already exists, skipping build"
            fi
          fi
          
          echo "Apptainer container build complete."
      
      - name: Clone HuggingFace Model
        if: ${{ inputs.model.source == 'huggingface' }}
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ inputs.advanced_settings.rundir }}
          
          MODEL_ID="${{ inputs.model.hf_model_id }}"
          CACHE_DIR="${{ inputs.model.cache_dir }}"
          HF_TOKEN="${{ inputs.model.hf_token }}"
          
          # Expand ~ in cache directory
          CACHE_DIR="${CACHE_DIR/#\~/$HOME}"
          
          # Create cache directory
          mkdir -p "$CACHE_DIR"
          
          # Sanitize model ID for directory name (replace / with __)
          SAFE_MODEL_ID="${MODEL_ID//\//__}"
          TARGET_DIR="$CACHE_DIR/$SAFE_MODEL_ID"
          
          # Check if model already exists
          if [[ -d "$TARGET_DIR" && -f "$TARGET_DIR/config.json" ]]; then
            echo "Model already cached at $TARGET_DIR"
            echo "Skipping clone - model weights already present"
            # Update .run.env with resolved path for offline loading
            echo "export MODEL_PATH=$TARGET_DIR" >> .run.env
            echo "export MODEL_NAME=$TARGET_DIR" >> .run.env
            echo "export TRANSFORMERS_OFFLINE=1" >> .run.env
            exit 0
          fi
          
          echo "Cloning model $MODEL_ID to $TARGET_DIR"
          
          # Ensure git is available
          if ! command -v git >/dev/null 2>&1; then
            echo "ERROR: git is not installed"
            exit 1
          fi
          
          # Install git-lfs if not available
          if ! git lfs version >/dev/null 2>&1; then
            echo "git-lfs not found, installing locally..."
            
            mkdir -p $HOME/bin
            cd /tmp
            
            # Download latest git-lfs release
            LFS_URL=$(curl -s https://api.github.com/repos/git-lfs/git-lfs/releases/latest \
              | grep browser_download_url \
              | grep linux-amd64 \
              | head -1 \
              | cut -d '"' -f 4)
            
            if [[ -z "$LFS_URL" ]]; then
              echo "ERROR: Could not find git-lfs download URL"
              exit 1
            fi
            
            wget -q "$LFS_URL" -O git-lfs-linux-amd64.tar.gz
            tar -xzf git-lfs-linux-amd64.tar.gz
            ./git-lfs-*/install.sh --local
            rm -rf git-lfs-* 
            
            # Add to PATH for this session
            export PATH="$HOME/bin:$PATH"
            
            cd ${{ inputs.advanced_settings.rundir }}
            
            if ! git lfs version >/dev/null 2>&1; then
              echo "ERROR: Failed to install git-lfs"
              exit 1
            fi
            echo "git-lfs installed successfully"
          fi
          
          # Initialize git-lfs
          git lfs install
          
          # Build clone URL (with token for gated models)
          REPO_URL="https://huggingface.co/$MODEL_ID"
          if [[ -n "$HF_TOKEN" ]]; then
            REPO_URL="https://user:${HF_TOKEN}@huggingface.co/$MODEL_ID"
          fi
          
          # Clone with depth 1 to save space/time
          echo "Cloning from HuggingFace Hub..."
          git clone --depth 1 "$REPO_URL" "$TARGET_DIR"
          
          # Verify clone
          if [[ ! -f "$TARGET_DIR/config.json" ]]; then
            echo "ERROR: Model clone incomplete - missing config.json"
            exit 1
          fi
          
          # Update .run.env with resolved path for offline loading
          echo "export MODEL_PATH=$TARGET_DIR" >> .run.env
          echo "export MODEL_NAME=$TARGET_DIR" >> .run.env
          echo "export TRANSFORMERS_OFFLINE=1" >> .run.env
          echo "Model clone complete: $TARGET_DIR"

      - name: Pull Tiktoken Encodings
        if: ${{ inputs.advanced_settings.tiktoken_encodings == true }}
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ inputs.advanced_settings.rundir }}
          mkdir -p cache/tiktoken_encodings
          wget -O cache/tiktoken_encodings/o200k_base.tiktoken "https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken"
          wget -O cache/tiktoken_encodings/cl100k_base.tiktoken "https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken"

  # ============================================================================
  # SLURM Job
  # ============================================================================
  slurm_job:
    needs:
      - prepare_job_directory
    if: ${{ inputs.execmethod == 'SLURM' }}
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    steps:
      - name: Create SLURM Script
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ inputs.advanced_settings.rundir }}
          echo '#!/bin/bash' > run.sh
          chmod +x run.sh
          
          # SLURM directives
          if [[ -n "${{ inputs.slurm.account }}" && "${{ inputs.slurm.account }}" != "undefined" ]]; then
            echo "#SBATCH --account=${{ inputs.slurm.account }}" >> run.sh
          fi
          if [[ -n "${{ inputs.slurm.qos }}" && "${{ inputs.slurm.qos }}" != "undefined" ]]; then
            echo "#SBATCH --qos=${{ inputs.slurm.qos }}" >> run.sh
          fi
          if [[ -n "${{ inputs.slurm.partition }}" && "${{ inputs.slurm.partition }}" != "undefined" ]]; then
            echo "#SBATCH --partition=${{ inputs.slurm.partition }}" >> run.sh
          fi
          if [[ -n "${{ inputs.slurm.cpus_per_task }}" ]]; then
            echo "#SBATCH --cpus-per-task=${{ inputs.slurm.cpus_per_task }}" >> run.sh
          fi
          echo "#SBATCH --nodes=1" >> run.sh
          if [[ -n "${{ inputs.slurm.time }}" ]]; then
            echo "#SBATCH --time=${{ inputs.slurm.time }}" >> run.sh
          fi
          echo "#SBATCH --chdir=${PWD}" >> run.sh
          echo "#SBATCH -o ${PWD}/run.out" >> run.sh
          echo "#SBATCH -e ${PWD}/run.out" >> run.sh
          
          # Additional scheduler directives
          if [[ -n "${{ inputs.slurm.scheduler_directives }}" && "${{ inputs.slurm.scheduler_directives }}" != "undefined" ]]; then
            echo "${{ inputs.slurm.scheduler_directives }}" >> run.sh
          fi
          
          # Job started marker
          echo "touch job.started" >> run.sh
          echo "hostname >> HOSTNAME" >> run.sh
          
          # Load singularity module if needed
          cat << 'EOF' >> run.sh
          
          # Ensure Singularity is available
          if ! command -v singularity >/dev/null 2>&1; then
            if command -v module >/dev/null 2>&1; then
              module load singularity 2>/dev/null || module load apptainer 2>/dev/null || true
            fi
          fi
          EOF
          
          cat start_service.sh >> run.sh
      
      - name: Submit SLURM Script
        run: |
          set -x
          cd ${{ inputs.advanced_settings.rundir }}
          echo "$(date) Submitting SLURM Job"
          jobid=$(sbatch run.sh | tail -1 | awk -F ' ' '{print $4}')
          if [ -z "${jobid}" ]; then
            echo "$(date) Job submission failed"
            exit 1
          fi
          echo "jobid=${jobid}" | tee -a $OUTPUTS | tee -a jobid
        cleanup: |
          set -x
          cd ${{ inputs.advanced_settings.rundir }}
          source jobid 2>/dev/null || true
          if [[ -n "$jobid" ]]; then
            target_hostname=$(squeue -j "${jobid}" --noheader --format="%N" 2>/dev/null)
            if [[ -n "$target_hostname" ]]; then
              ssh ${target_hostname} "cd ${{ inputs.advanced_settings.rundir }} && bash cancel.sh" 2>/dev/null || true
            fi
            scancel ${jobid} 2>/dev/null || true
          fi
          rm -f jobid SESSION_PORT job.started HOSTNAME
      
      - name: Monitor SLURM Job
        run: |
          source jobid 2>/dev/null || true
          echo "$(date) Monitoring SLURM job ${jobid}"

          cd ${{ inputs.advanced_settings.rundir }}
          touch run.out
          tail -f run.out &
          echo $! > tail.pid

          get_slurm_job_status() {
              if [ -z "${SQUEUE_HEADER}" ]; then
                  export SQUEUE_HEADER="$(squeue 2>/dev/null | awk 'NR==1')"
              fi
              status_column=$(echo "${SQUEUE_HEADER}" | awk '{ for (i=1; i<=NF; i++) if ($i ~ /^S/) { print i; exit } }')
              status_response=$(squeue 2>/dev/null | awk -v jobid="${jobid}" '$1 == jobid')
              echo "${SQUEUE_HEADER}"
              echo "${status_response}"
              export job_status=$(echo ${status_response} | awk -v id="${jobid}" -v col="$status_column" '{print $col}')
          }

          while true; do
            sleep 15
            get_slurm_job_status
            if [ -z "${job_status}" ]; then
              job_status=$(sacct -j ${jobid} --format=state 2>/dev/null | tail -n1)
              echo "$(date) Job exited with status ${job_status}"
              touch job.ended
              exit 0
            fi
          done
        cleanup: |
          set -x
          cd ${{ inputs.advanced_settings.rundir }}
          if [ -f tail.pid ]; then
            kill $(cat tail.pid) 2>/dev/null || true
            rm -f tail.pid
          fi

  # ============================================================================
  # PBS Job
  # ============================================================================
  pbs_job:
    needs:
      - prepare_job_directory
    if: ${{ inputs.execmethod == 'PBS' }}
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    steps:
      - name: Create PBS Script
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ inputs.advanced_settings.rundir }}
          
          # Write PBS directives from user input
          cat << 'PBSEOF' > run.sh
          ${{ inputs.pbs.scheduler_directives }}
          PBSEOF
          chmod +x run.sh
          
          # Add account if specified
          if [[ -n "${{ inputs.pbs.account }}" && "${{ inputs.pbs.account }}" != "undefined" ]]; then
            sed -i "2i #PBS -A ${{ inputs.pbs.account }}" run.sh
          fi
          
          # Job started marker
          echo "" >> run.sh
          echo "cd ${{ inputs.advanced_settings.rundir }}" >> run.sh
          echo "touch job.started" >> run.sh
          echo "hostname >> HOSTNAME" >> run.sh
          
          # Load singularity module if needed
          cat << 'EOF' >> run.sh
          
          # Ensure Singularity is available
          if ! command -v singularity >/dev/null 2>&1; then
            if command -v module >/dev/null 2>&1; then
              module load singularity 2>/dev/null || module load apptainer 2>/dev/null || true
            fi
          fi
          EOF
          
          cat start_service.sh >> run.sh
      
      - name: Submit PBS Script
        run: |
          set -x
          cd ${{ inputs.advanced_settings.rundir }}
          echo "$(date) Submitting PBS Job"
          jobid=$(qsub run.sh)
          if [ -z "${jobid}" ]; then
            echo "$(date) Job submission failed"
            exit 1
          fi
          echo "jobid=${jobid}" | tee -a $OUTPUTS | tee -a jobid
        cleanup: |
          set -x
          cd ${{ inputs.advanced_settings.rundir }}
          source jobid 2>/dev/null || true
          if [[ -n "$jobid" ]]; then
            qdel ${jobid} 2>/dev/null || true
          fi
          rm -f jobid SESSION_PORT job.started HOSTNAME
      
      - name: Monitor PBS Job
        run: |
          source jobid 2>/dev/null || true
          echo "$(date) Monitoring PBS job ${jobid}"

          cd ${{ inputs.advanced_settings.rundir }}
          touch run.out
          
          # PBS job monitoring loop
          while true; do
            sleep 15
            
            # Check job status
            job_status=$(qstat -f ${jobid} 2>/dev/null | grep "job_state" | awk '{print $3}')
            echo "$(date) PBS job status: ${job_status}"
            
            if [ -z "${job_status}" ] || [[ "${job_status}" == "C" ]] || [[ "${job_status}" == "E" ]]; then
              echo "$(date) Job completed or not found"
              touch job.ended
              exit 0
            fi
            
            # If job is running, try to tail output
            if [[ "${job_status}" == "R" ]] && [ -f run.out ]; then
              tail -f run.out &
              wait $!
            fi
          done

  # ============================================================================
  # SSH Job (Direct Execution)
  # ============================================================================
  ssh_job:
    needs:
      - prepare_job_directory
    if: ${{ inputs.execmethod == 'SSH' }}
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    steps:
      - name: Create SSH Script
        early-cancel: any-job-failed
        run: |
          cd ${{ inputs.advanced_settings.rundir }}
          echo '#!/bin/bash' > run.sh
          chmod +x run.sh
          
          echo "touch job.started" >> run.sh
          echo "hostname >> HOSTNAME" >> run.sh
          
          cat start_service.sh >> run.sh
      
      - name: Submit SSH Script
        run: |
          cd ${{ inputs.advanced_settings.rundir }}
          bash ./run.sh
          touch job.ended
          rm -f jobid SESSION_PORT job.started HOSTNAME
        cleanup: |
          set -x
          cd ${{ inputs.advanced_settings.rundir }}
          bash cancel.sh 2>/dev/null || true

  # ============================================================================
  # Session Management
  # ============================================================================
  create_session:
    needs:
      - prepare_job_directory
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    steps:
      - name: Wait for job to start
        early-cancel: any-job-failed
        run: |
          set -x
          timeout=600
          elapsed=0
          while [ ! -f ${{ inputs.advanced_settings.rundir }}/job.started ]; do
            echo "Waiting for job to start... (${elapsed}s/${timeout}s)"
            sleep 5
            ((elapsed+=5))
            if [ $elapsed -ge $timeout ]; then
              echo "ERROR: Timeout waiting for job to start"
              exit 1
            fi
          done
      
      - name: Get Hostname
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ inputs.advanced_settings.rundir }}
          
          if [[ "${{ inputs.execmethod }}" == "SLURM" ]]; then
            source jobid 2>/dev/null || true
            target_hostname=$(squeue -j "${jobid}" --noheader --format="%N" 2>/dev/null)
          elif [[ "${{ inputs.execmethod }}" == "PBS" ]]; then
            # Wait for HOSTNAME file to be created
            timeout=120
            elapsed=0
            while [ ! -f HOSTNAME ]; do
              sleep 2
              ((elapsed+=2))
              if [ $elapsed -ge $timeout ]; then break; fi
            done
            target_hostname=$(cat HOSTNAME 2>/dev/null | head -1)
          elif [[ "${{ inputs.execmethod }}" == "SSH" ]]; then
            target_hostname=$(hostname)
          fi
          
          if [ -z "${target_hostname}" ]; then
            echo "$(date) Failed to get target hostname"
            exit 1
          fi
          
          echo "target_hostname=${target_hostname}" | tee -a $OUTPUTS
      
      - name: Get Session Port
        early-cancel: any-job-failed
        run: |
          set -euo pipefail
          set -x

          TIMEOUT=600
          RETRY_INTERVAL=3
          cd ${{ inputs.advanced_settings.rundir }}

          attempt=1
          elapsed=0
          while true; do
              echo "$(date) Attempt $attempt: Checking for SESSION_PORT file..."

              if [ -f SESSION_PORT ]; then
                  echo "$(date) Success: SESSION_PORT file found!"
                  cat SESSION_PORT | tee -a "$OUTPUTS"
                  exit 0
              elif [ -f job.ended ]; then
                  echo "$(date) Job completed but SESSION_PORT was never created. Exiting..."
                  exit 1
              else
                  echo "$(date) SESSION_PORT not found. Retrying in ${RETRY_INTERVAL} seconds..."
                  sleep "$RETRY_INTERVAL"
                  ((attempt++))
                  ((elapsed+=RETRY_INTERVAL))
                  if [ $elapsed -ge $TIMEOUT ]; then
                    echo "$(date) Timeout waiting for SESSION_PORT"
                    exit 1
                  fi
              fi
          done
      
      - name: Wait for Server To Start
        early-cancel: any-job-failed
        run: |
          TIMEOUT=300
          RETRY_INTERVAL=3
          remote_host="${{ needs.create_session.outputs.target_hostname }}"
          remote_port="${{ needs.create_session.outputs.SESSION_PORT }}"

          check_server() {
              curl --silent --connect-timeout 5 "http://${remote_host}:${remote_port}" >/dev/null 2>&1
              return $?
          }

          cd ${{ inputs.advanced_settings.rundir }}

          attempt=1
          elapsed=0
          while true; do
              echo "$(date) Attempt $attempt: Checking if server is listening on ${remote_host}:${remote_port}..."
              
              if check_server; then
                  echo "$(date) Success: Server is listening on ${remote_host}:${remote_port}!"
                  exit 0
              elif [ -f job.ended ]; then
                  echo "$(date) Job completed. Exiting..."
                  exit 0
              else
                  echo "$(date) Server not responding. Retrying in ${RETRY_INTERVAL} seconds..."
                  sleep "$RETRY_INTERVAL"
                  ((attempt++))
                  ((elapsed+=RETRY_INTERVAL))
                  if [ $elapsed -ge $TIMEOUT ]; then
                    echo "$(date) Timeout waiting for server"
                    exit 1
                  fi
              fi
          done
      
      - name: Update Session
        uses: parallelworks/update-session
        with:
          remotePort: '${{ needs.create_session.outputs.SESSION_PORT }}'
          target: '${{ inputs.resource.id }}'
          name: '${{ sessions.session }}'
          remoteHost: '${{ needs.create_session.outputs.target_hostname }}'
          localPort: '${{ inputs.advanced_settings.localport }}'

# ==============================================================================
# INPUT DEFINITIONS
# ==============================================================================
'on':
  execute:
    inputs:
      # ========================================================================
      # Resource Selection
      # ========================================================================
      resource:
        type: compute-clusters
        label: Compute Cluster
        autoselect: true
        include-workspace: false
        tooltip: Resource to run the service
      
      # ========================================================================
      # Execution Settings
      # ========================================================================
      execmethod:
        type: dropdown
        label: Execution Method
        default: SLURM
        tooltip: How to run the job - SSH (direct), SLURM, or PBS
        options:
          - value: SSH
            label: SSH (Direct)
          - value: SLURM
            label: SLURM
          - value: PBS
            label: PBS
      
      runmode:
        label: Container Runtime
        type: dropdown
        default: singularity
        tooltip: Apptainer/Singularity is recommended for HPC environments
        options:
          - value: singularity
            label: Apptainer
          - value: docker
            label: Docker
      
      runtype:
        label: Deployment Type
        type: dropdown
        default: vllm
        tooltip: Deploy vLLM only or full vLLM+RAG stack
        options:
          - value: vllm
            label: vLLM Only
          - value: all
            label: vLLM + RAG (Full Stack)

      # ========================================================================
      # Model Configuration
      # ========================================================================
      model:
        type: group
        label: Model Configuration
        items:
          source:
            type: dropdown
            label: Model Source
            default: huggingface
            tooltip: Use a pre-downloaded model (local) or clone from HuggingFace using git-lfs
            options:
              - value: local
                label: "üìÅ Local Path (pre-staged weights)"
              - value: huggingface
                label: "ü§ó HuggingFace Clone (git-lfs)"
          
          local_path:
            type: string
            label: Model Path
            placeholder: /path/to/model/weights
            default: /models/Llama-3_3-Nemotron-Super-49B-v1_5
            tooltip: Full path to directory containing model weights
            hidden: ${{ inputs.model.source != 'local' }}
            ignore: ${{ inputs.model.source != 'local' }}
          
          hf_model_id:
            type: string
            label: HuggingFace Model ID
            placeholder: nvidia/Llama-3_3-Nemotron-Super-49B-v1_5
            tooltip: Model ID to clone from HuggingFace (e.g., nvidia/Llama-3_3-Nemotron-Super-49B-v1_5). Uses git-lfs for HPC compatibility.
            hidden: ${{ inputs.model.source != 'huggingface' }}
            ignore: ${{ inputs.model.source != 'huggingface' }}
          
          hf_token:
            label: HuggingFace Token
            optional: true
            default: ${{ org.HF_TOKEN }}
            type: password
            tooltip: Required for gated models (Llama, etc.)
            hidden: ${{ inputs.model.source != 'huggingface' }}
            ignore: ${{ inputs.model.source != 'huggingface' }}
          
          cache_dir:
            type: string
            label: Model Cache Directory
            default: ~/pw/models
            tooltip: Directory to clone model into. Model is cloned once and reused for subsequent runs. Ensure sufficient disk space.
            hidden: ${{ inputs.model.source != 'huggingface' }}
            ignore: ${{ inputs.model.source != 'huggingface' }}

      # ========================================================================
      # vLLM Configuration
      # ========================================================================
      vllm:
        type: group
        label: vLLM Settings
        collapsed: true
        items:
          num_gpus:
            type: dropdown
            label: Number of GPUs
            default: "4"
            tooltip: Number of GPUs for tensor parallelism (must match available GPUs)
            options:
              - value: "1"
                label: "1 GPU"
              - value: "2"
                label: "2 GPUs"
              - value: "4"
                label: "4 GPUs"
              - value: "8"
                label: "8 GPUs"
          
          gpu_memory:
            type: dropdown
            label: GPU Memory Utilization
            default: "0.85"
            tooltip: Fraction of GPU memory to use (lower if OOM errors occur)
            options:
              - value: "0.95"
                label: "95% (Maximum)"
              - value: "0.90"
                label: "90%"
              - value: "0.85"
                label: "85% (Recommended)"
              - value: "0.80"
                label: "80%"
              - value: "0.70"
                label: "70% (Conservative)"
          
          max_model_len:
            type: dropdown
            label: Max Context Length
            default: "auto"
            tooltip: Maximum sequence length. Lower values reduce memory usage.
            options:
              - value: "auto"
                label: "Auto (use model default)"
              - value: "4096"
                label: "4K tokens"
              - value: "8192"
                label: "8K tokens"
              - value: "16384"
                label: "16K tokens"
              - value: "32768"
                label: "32K tokens"
              - value: "65536"
                label: "64K tokens"
              - value: "131072"
                label: "128K tokens"
          
          dtype:
            type: dropdown
            label: Data Type
            default: bfloat16
            tooltip: Model precision. Select 'Custom' to specify via extra arguments.
            options:
              - value: bfloat16
                label: "bfloat16 (Recommended for A100/H100)"
              - value: float16
                label: "float16 (Better compatibility)"
              - value: auto
                label: "Auto (model default)"
              - value: custom
                label: "Custom (specify in extra args)"
          
          extra_args:
            type: string
            label: Additional Arguments
            default: "--trust_remote_code --async-scheduling"
            placeholder: "--trust_remote_code"
            tooltip: Additional vLLM arguments. If dtype is 'Custom', include --dtype here.
            optional: true

      # ========================================================================
      # RAG Configuration
      # ========================================================================
      rag:
        type: group
        label: RAG Settings
        hidden: ${{ inputs.runtype != 'all' }}
        collapsed: true
        items:
          docsdir:
            label: Documents Directory
            optional: true
            default: ./docs
            type: string
            tooltip: Directory containing documents to index for RAG
          
          systemprompt:
            type: string
            label: System Prompt
            textarea: true
            optional: true
            default: You are a careful assistant. Use ONLY the provided context blocks to answer. Each block is numbered [1], [2], ‚Ä¶ and includes source metadata. When you use information from a block, you MUST cite it inline with [n]. At the end of your response, include a 'References' section with one reference per line formatted as [n] file_path (chunk index). Do not invent citations or sources. If the context does not contain the answer, say so briefly.

      # ========================================================================
      # Container Configuration
      # ========================================================================
      container:
        type: group
        label: Container Options
        hidden: ${{ inputs.runmode != 'singularity' }}
        collapsed: true
        items:
          source:
            type: dropdown
            label: Container Source
            default: pull
            tooltip: How to obtain Apptainer/Singularity containers
            options:
              - value: pull
                label: "üì• Pull from bucket (recommended)"
              - value: path
                label: "üìÇ Use existing path"
              - value: build
                label: "üî® Build from source (requires sudo/fakeroot)"
          
          bucket:
            label: Container Bucket
            type: string
            default: pw://mshaxted/codeassist
            tooltip: Bucket containing vllm.sif and rag.sif containers
            hidden: ${{ inputs.container.source != 'pull' }}
            ignore: ${{ inputs.container.source != 'pull' }}
          
          vllm_path:
            label: vLLM Container Path
            type: string
            placeholder: /path/to/vllm.sif
            tooltip: Full path to existing vllm.sif container
            hidden: ${{ inputs.container.source != 'path' }}
            ignore: ${{ inputs.container.source != 'path' }}
          
          rag_path:
            label: RAG Container Path
            type: string
            placeholder: /path/to/rag.sif
            tooltip: Full path to existing rag.sif container (only needed for vLLM+RAG mode)
            hidden: ${{ inputs.container.source != 'path' || inputs.runtype != 'all' }}
            ignore: ${{ inputs.container.source != 'path' || inputs.runtype != 'all' }}

      # ========================================================================
      # SLURM Configuration
      # ========================================================================
      slurm:
        type: group
        label: SLURM Options
        hidden: ${{ inputs.execmethod != 'SLURM' }}
        collapsed: true
        items:
          account:
            label: Account
            type: slurm-accounts
            resource: ${{ inputs.resource }}
            tooltip: SLURM account for job submission
            ignore: ${{ inputs.execmethod != 'SLURM' }}
            optional: true
          
          partition:
            type: slurm-partitions
            label: Partition
            ignore: ${{ inputs.execmethod != 'SLURM' }}
            optional: true
            resource: ${{ inputs.resource }}
            tooltip: Partition for job submission. Leave empty for default.
          
          qos:
            label: Quality of Service (QoS)
            type: slurm-qos
            resource: ${{ inputs.resource }}
            tooltip: SLURM QoS setting
            ignore: ${{ inputs.execmethod != 'SLURM' }}
            optional: true
          
          cpus_per_task:
            type: number
            label: CPUs per Task
            ignore: ${{ inputs.execmethod != 'SLURM' }}
            min: 1
            max: 128
            default: 4
            tooltip: Number of CPUs for the job
          
          time:
            label: Walltime
            type: string
            default: 04:00:00
            ignore: ${{ inputs.execmethod != 'SLURM' }}
            tooltip: Maximum job duration (e.g., 04:00:00)
          
          scheduler_directives:
            type: editor
            label: Additional Directives
            ignore: ${{ inputs.execmethod != 'SLURM' }}
            optional: true
            tooltip: Additional SLURM directives
            default: |
              #SBATCH --gres=gpu:4
              
              ##SBATCH --constraint=mla  # Uncomment for Navy/AFRL DSRC systems

      # ========================================================================
      # PBS Configuration
      # ========================================================================
      pbs:
        type: group
        label: PBS Options
        hidden: ${{ inputs.execmethod != 'PBS' }}
        collapsed: true
        items:
          account:
            label: Account
            type: string
            ignore: ${{ inputs.execmethod != 'PBS' }}
            optional: true
            tooltip: PBS account for job submission
          
          scheduler_directives:
            label: PBS Script
            type: editor
            tooltip: Full PBS script including directives
            ignore: ${{ inputs.execmethod != 'PBS' }}
            optional: ${{ inputs.execmethod != 'PBS' }}
            default: |
              #!/bin/bash
              #PBS -q gpu
              #PBS -l select=1:ncpus=92:mpiprocs=1:ngpus=4
              #PBS -l walltime=04:00:00
              #PBS -V

      # ========================================================================
      # Advanced Settings
      # ========================================================================
      advanced_settings:
        type: group
        label: Advanced Settings
        collapsed: true
        items:
          localport:
            label: Local Port
            default: '5555'
            tooltip: Port in your workspace to access the service
            type: string
          
          apikey:
            label: vLLM API Key
            optional: true
            tooltip: Optional API key for vLLM server. Can be any value (e.g., 'dummy') - Cline and other IDEs require a value but do not validate it.
            type: password
          
          rundir:
            label: Run Directory
            default: ~/pw/activate-rag-vllm
            type: string
            tooltip: Directory where the service will be deployed
          
          repository:
            type: string
            label: Repository
            default: https://github.com/parallelworks/activate-rag-vllm.git
          
          repository_branch:
            type: string
            label: Repository Branch
            default: refactor
          
          tiktoken_encodings:
            label: Download Tiktoken Encodings
            tooltip: Download tiktoken encodings for offline use (required for some models)
            type: boolean
            default: false
          
          vllm_attention_backend:
            type: dropdown
            label: vLLM Attention Backend
            default: FLASH_ATTN
            tooltip: Attention implementation used by vLLM
            options:
              - value: FLASH_ATTN
              - value: TRITON_ATTN
              - value: FLASHINFER
              - value: FLASHINFER_MLA
              - value: TRITON_MLA
              - value: CUTLASS_MLA
              - value: FLASHMLA
              - value: FLASHMLA_SPARSE
              - value: FLASH_ATTN_MLA
              - value: TORCH_SDPA
              - value: ROCM_ATTN
              - value: ROCM_AITER_MLA
              - value: ROCM_AITER_TRITON_MLA
              - value: ROCM_AITER_FA
              - value: PALLAS
              - value: IPEX
              - value: CPU_ATTN
              - value: NO_ATTENTION
              - value: CUSTOM
