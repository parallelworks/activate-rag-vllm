# yaml-language-server: $schema=https://activate.parallel.works/workflow.schema.json
---
# ==============================================================================
# ACTIVATE RAG + vLLM Deployment Workflow
# ==============================================================================
# Deploys vLLM inference server with optional RAG (Retrieval-Augmented Generation)
# capabilities on HPC clusters using Apptainer/Singularity containers.
#
# Features:
#   - vLLM inference with tensor parallelism for multi-GPU deployments
#   - Optional RAG stack with ChromaDB vector store and document indexing
#   - HuggingFace model cloning via git-lfs or local model path support
#   - OpenAI-compatible API endpoint for IDE integration (Cline, Continue, etc.)
#   - Unified SLURM/PBS/SSH job submission via job_runner marketplace action
#
# Organized Input Sections:
#   1. Resource Selection          - Compute cluster target
#   2. Scheduler Options           - SLURM/PBS configuration for HPC execution
#   3. Container Runtime           - Apptainer/Singularity or Docker
#   4. Model Configuration         - HuggingFace or local model paths
#   5. vLLM Settings               - GPU count, memory, context length, dtype
#   6. RAG Settings                - Documents directory, system prompt
#   7. Container Options           - Container source (path, pull, build)
#   8. Advanced Settings           - Ports, API keys, repository branch
#
# Usage:
#   Deploy vLLM-only for code assistance or full vLLM+RAG stack for document QA.
#   Connects to VS Code/IDE via OpenAI-compatible session endpoint.
# ==============================================================================

permissions:
  - '*'
sessions:
  session:
    redirect: false
    openAI: true

# ==============================================================================
# JOB DEFINITIONS
# ==============================================================================
jobs:
  # ============================================================================
  # Setup: Clone Repository and Create Environment
  # ============================================================================
  setup:
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    outputs:
      rundir: ${{ needs.setup.steps.clone_repo.outputs.rundir }}
    steps:
      - name: Clone Repository
        id: clone_repo
        early-cancel: any-job-failed
        run: |
          set -x
          RUNDIR="${{ inputs.advanced_settings.rundir }}"
          RUNDIR="${RUNDIR/#\~/$HOME}"
          
          mkdir -p "$(dirname $RUNDIR)"
          
          if [[ -d "$RUNDIR/.git" ]]; then
            echo "Repository exists, updating..."
            cd "$RUNDIR"
            git fetch origin
            git checkout ${{ inputs.advanced_settings.repository_branch }}
            git reset --hard origin/${{ inputs.advanced_settings.repository_branch }}
          else
            echo "Cloning fresh repository..."
            git clone -b ${{ inputs.advanced_settings.repository_branch }} ${{ inputs.advanced_settings.repository }} "$RUNDIR"
          fi
          
          cd "$RUNDIR"
          rm -f jobid SESSION_PORT job.started job.ended run.out HOSTNAME
          rm -rf logs
          
          # Create cache directories (needed for bind mounts even if not used)
          mkdir -p cache/tiktoken_encodings
          
          echo "rundir=$RUNDIR" >> $OUTPUTS
      
      - name: Create Environment File
        id: create_env
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ needs.setup.steps.clone_repo.outputs.rundir }}
          
          # Core configuration
          cat > .run.env << 'ENVEOF'
          export RUNMODE=${{ inputs.runmode }}
          export RUNTYPE=${{ inputs.runtype }}
          export SYSTEM_PROMPT="${{ inputs.rag.systemprompt }}"
          export HF_TOKEN=${{ inputs.model.hf_token }}
          export API_KEY=${{ inputs.advanced_settings.apikey }}
          export DOCS_DIR=${{ inputs.rag.docsdir }}
          ENVEOF
          
          # Build vLLM extra args from structured inputs
          if [[ "${{ inputs.vllm.dtype }}" == "custom" ]]; then
            VLLM_ARGS="--tensor-parallel-size ${{ inputs.vllm.num_gpus }} --gpu-memory-utilization ${{ inputs.vllm.gpu_memory }}"
          else
            VLLM_ARGS="--dtype ${{ inputs.vllm.dtype }} --tensor-parallel-size ${{ inputs.vllm.num_gpus }} --gpu-memory-utilization ${{ inputs.vllm.gpu_memory }}"
          fi
          [[ "${{ inputs.vllm.max_model_len }}" != "auto" ]] && VLLM_ARGS="$VLLM_ARGS --max-model-len ${{ inputs.vllm.max_model_len }}"
          [[ -n "${{ inputs.vllm.extra_args }}" ]] && VLLM_ARGS="$VLLM_ARGS ${{ inputs.vllm.extra_args }}"
          echo "export VLLM_EXTRA_ARGS=\"$VLLM_ARGS\"" >> .run.env
          
          # Model configuration
          echo "export MODEL_SOURCE=${{ inputs.model.source }}" >> .run.env
          if [[ "${{ inputs.model.source }}" == "local" ]]; then
            echo "export MODEL_NAME=${{ inputs.model.local_path }}" >> .run.env
            echo "export MODEL_PATH=${{ inputs.model.local_path }}" >> .run.env
            echo "export TRANSFORMERS_OFFLINE=1" >> .run.env
          else
            echo "export MODEL_NAME=${{ inputs.model.hf_model_id }}" >> .run.env
            echo "export HF_MODEL_ID=${{ inputs.model.hf_model_id }}" >> .run.env
            echo "export MODEL_CACHE_BASE=${{ inputs.model.cache_dir }}" >> .run.env
          fi
          
          # Container paths (used for both 'path' and 'build' modes)
          if [[ "${{ inputs.container.source }}" != "pull" ]]; then
            echo "export VLLM_CONTAINER_PATH=${{ inputs.container.vllm_path }}" >> .run.env
            echo "export RAG_CONTAINER_PATH=${{ inputs.container.rag_path }}" >> .run.env
          fi
          
          # Additional settings
          [[ -n "${{ inputs.advanced_settings.vllm_attention_backend }}" ]] && echo "export VLLM_ATTENTION_BACKEND=${{ inputs.advanced_settings.vllm_attention_backend }}" >> .run.env
          [[ "${{ inputs.advanced_settings.tiktoken_encodings }}" == "true" ]] && echo "export TIKTOKEN_ENCODINGS_BASE=/root/.cache/tiktoken_encodings" >> .run.env
          
          echo "Environment file created"

  # ============================================================================
  # Prepare Containers (Apptainer/Singularity)
  # ============================================================================
  prepare_containers:
    needs: [setup]
    if: ${{ inputs.runmode == 'singularity' }}
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    steps:
      - name: Install Apptainer Compose
        id: install_compose
        early-cancel: any-job-failed
        run: |
          set -x
          if ! command -v singularity-compose &> /dev/null; then
            if [ -d ~/pw/software/singularity-compose ]; then
              source ~/pw/software/singularity-compose/bin/activate
            fi
            if ! command -v singularity-compose &> /dev/null; then
              echo "$(date) Installing singularity-compose..."
              mkdir -p ~/pw/software
              python3 -m venv ~/pw/software/singularity-compose
              source ~/pw/software/singularity-compose/bin/activate
              pip install --upgrade pip
              pip install singularity-compose
            fi
          fi
          command -v singularity-compose >/dev/null 2>&1 || { echo "ERROR: Failed to install singularity-compose"; exit 1; }
          echo "singularity-compose ready"
      
      - name: Pull Containers from Bucket
        id: pull_containers
        if: ${{ inputs.container.source == 'pull' }}
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ needs.setup.outputs.rundir }}
          
          [[ ! -f "vllm.sif" ]] && pw bucket cp "${{ inputs.container.bucket }}/vllm.sif" ./ || echo "vllm.sif exists"
          [[ "${{ inputs.runtype }}" == "all" && ! -f "rag.sif" ]] && pw bucket cp "${{ inputs.container.bucket }}/rag.sif" ./ || echo "rag.sif exists or not needed"
          
          echo "Container pull complete"
      
      - name: Link Existing Containers
        id: link_containers
        if: ${{ inputs.container.source == 'path' }}
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ needs.setup.outputs.rundir }}
          
          VLLM_PATH="${{ inputs.container.vllm_path }}"
          VLLM_PATH="${VLLM_PATH/#\~/$HOME}"
          
          [[ -f "$VLLM_PATH" ]] && ln -sf "$VLLM_PATH" ./vllm.sif || { echo "ERROR: vLLM container not found at $VLLM_PATH"; exit 1; }
          echo "Linked vllm.sif from $VLLM_PATH"
          
          if [[ "${{ inputs.runtype }}" == "all" ]]; then
            RAG_PATH="${{ inputs.container.rag_path }}"
            RAG_PATH="${RAG_PATH/#\~/$HOME}"
            [[ -f "$RAG_PATH" ]] && ln -sf "$RAG_PATH" ./rag.sif || { echo "ERROR: RAG container not found at $RAG_PATH"; exit 1; }
            echo "Linked rag.sif from $RAG_PATH"
          fi
      
      - name: Build Containers from Source
        id: build_containers
        if: ${{ inputs.container.source == 'build' }}
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ needs.setup.outputs.rundir }}
          
          # Resolve container paths (expand ~ to $HOME)
          VLLM_PATH="${{ inputs.container.vllm_path }}"
          VLLM_PATH="${VLLM_PATH/#\~/$HOME}"
          RAG_PATH="${{ inputs.container.rag_path }}"
          RAG_PATH="${RAG_PATH/#\~/$HOME}"
          
          echo "Building Apptainer containers (requires sudo or fakeroot)..."
          echo "vLLM container will be built to: $VLLM_PATH"
          echo "RAG container will be built to: $RAG_PATH"
          
          build_container() {
            local target=$1 def=$2
            [[ -f "$target" ]] && { echo "$target exists, skipping"; return 0; }
            echo "Building $target from $def..."
            mkdir -p "$(dirname "$target")"
            if sudo -n true 2>/dev/null; then
              sudo singularity build "$target" "$def"
            else
              singularity build --fakeroot "$target" "$def"
            fi
          }
          
          build_container "$VLLM_PATH" singularity/Singularity.vllm
          [[ "${{ inputs.runtype }}" == "all" ]] && build_container "$RAG_PATH" singularity/Singularity.rag
          
          # Create symlinks in rundir for consistency with other modes
          ln -sf "$VLLM_PATH" ./vllm.sif
          [[ "${{ inputs.runtype }}" == "all" ]] && ln -sf "$RAG_PATH" ./rag.sif
          
          echo "Container build complete"

  # ============================================================================
  # Prepare Model
  # ============================================================================
  prepare_model:
    needs: [setup]
    if: ${{ inputs.model.source == 'huggingface' }}
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    outputs:
      model_path: ${{ needs.prepare_model.steps.clone_model.outputs.model_path }}
    steps:
      - name: Clone HuggingFace Model
        id: clone_model
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ needs.setup.outputs.rundir }}
          
          MODEL_ID="${{ inputs.model.hf_model_id }}"
          CACHE_DIR="${{ inputs.model.cache_dir }}"
          CACHE_DIR="${CACHE_DIR/#\~/$HOME}"
          HF_TOKEN="${{ inputs.model.hf_token }}"
          
          mkdir -p "$CACHE_DIR"
          SAFE_MODEL_ID="${MODEL_ID//\//__}"
          TARGET_DIR="$CACHE_DIR/$SAFE_MODEL_ID"
          
          # Check if already cached
          if [[ -d "$TARGET_DIR" && -f "$TARGET_DIR/config.json" ]]; then
            echo "Model already cached at $TARGET_DIR"
          else
            echo "Cloning model $MODEL_ID to $TARGET_DIR"
            
            # Ensure git-lfs is available
            if ! git lfs version >/dev/null 2>&1; then
              echo "Installing git-lfs locally..."
              mkdir -p $HOME/bin
              cd /tmp
              LFS_URL=$(curl -s https://api.github.com/repos/git-lfs/git-lfs/releases/latest | grep browser_download_url | grep linux-amd64 | head -1 | cut -d '"' -f 4)
              [[ -z "$LFS_URL" ]] && { echo "ERROR: Could not find git-lfs download URL"; exit 1; }
              wget -q "$LFS_URL" -O git-lfs-linux-amd64.tar.gz
              tar -xzf git-lfs-linux-amd64.tar.gz
              ./git-lfs-*/install.sh --local
              rm -rf git-lfs-*
              export PATH="$HOME/bin:$PATH"
              cd ${{ needs.setup.outputs.rundir }}
              git lfs version >/dev/null 2>&1 || { echo "ERROR: Failed to install git-lfs"; exit 1; }
            fi
            
            git lfs install
            
            # Build clone URL
            REPO_URL="https://huggingface.co/$MODEL_ID"
            [[ -n "$HF_TOKEN" ]] && REPO_URL="https://user:${HF_TOKEN}@huggingface.co/$MODEL_ID"
            
            git clone --depth 1 "$REPO_URL" "$TARGET_DIR"
            [[ ! -f "$TARGET_DIR/config.json" ]] && { echo "ERROR: Model clone incomplete"; exit 1; }
          fi
          
          # Update environment file
          cat >> .run.env << EOF
          export MODEL_PATH=$TARGET_DIR
          export MODEL_NAME=$TARGET_DIR
          export TRANSFORMERS_OFFLINE=1
          EOF
          
          echo "model_path=$TARGET_DIR" >> $OUTPUTS
          echo "Model ready at $TARGET_DIR"

  # ============================================================================
  # Prepare Tiktoken Encodings (Optional)
  # ============================================================================
  prepare_tiktoken:
    needs: [setup]
    if: ${{ inputs.advanced_settings.tiktoken_encodings == true }}
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    steps:
      - name: Download Tiktoken Encodings
        id: download_tiktoken
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ needs.setup.outputs.rundir }}
          mkdir -p cache/tiktoken_encodings
          wget -O cache/tiktoken_encodings/o200k_base.tiktoken "https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken"
          wget -O cache/tiktoken_encodings/cl100k_base.tiktoken "https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken"
          echo "Tiktoken encodings downloaded"

  # ============================================================================
  # Run Service (using marketplace job_runner)
  # ============================================================================
  run_service:
    needs: [setup, prepare_containers, prepare_model, prepare_tiktoken]
    working-directory: ${{ needs.setup.outputs.rundir }}
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    steps:
      - name: Run Service
        uses: marketplace/job_runner/v4.0
        early-cancel: any-job-failed
        with:
          resource: ${{ inputs.resource }}
          shebang: '#!/bin/bash'
          rundir: ${{ needs.setup.outputs.rundir }}
          use_existing_script: true
          script_path: ${{ needs.setup.outputs.rundir }}/start_service.sh
          scheduler: ${{ inputs.scheduler.enabled }}
          inject_markers: true
          slurm:
            is_disabled: ${{ inputs.scheduler.slurm.is_disabled }}
            account: ${{ inputs.scheduler.slurm.account }}
            partition: ${{ inputs.scheduler.slurm.partition }}
            qos: ${{ inputs.scheduler.slurm.qos }}
            time: ${{ inputs.scheduler.slurm.time }}
            cpus_per_task: ${{ inputs.scheduler.slurm.cpus_per_task }}
            gres: ${{ inputs.scheduler.slurm.gres }}
            scheduler_directives: |
              ${{ inputs.scheduler.slurm.scheduler_directives }}
          pbs:
            is_disabled: ${{ inputs.scheduler.pbs.is_disabled }}
            account: ${{ inputs.scheduler.pbs.account }}
            queue: ${{ inputs.scheduler.pbs.queue }}
            walltime: ${{ inputs.scheduler.pbs.walltime }}
            select: ${{ inputs.scheduler.pbs.select }}
            scheduler_directives: |
              ${{ inputs.scheduler.pbs.scheduler_directives }}

  # ============================================================================
  # Session Management
  # ============================================================================
  create_session:
    needs: [setup]
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    outputs:
      target_hostname: ${{ needs.create_session.steps.get_hostname.outputs.target_hostname }}
      SESSION_PORT: ${{ needs.create_session.steps.get_port.outputs.SESSION_PORT }}
    steps:
      - name: Wait for Job to Start
        id: wait_job
        early-cancel: any-job-failed
        run: |
          set -x
          timeout=600
          elapsed=0
          while [ ! -f ${{ needs.setup.outputs.rundir }}/job.started ]; do
            echo "Waiting for job to start... (${elapsed}s/${timeout}s)"
            sleep 5
            ((elapsed+=5))
            [[ $elapsed -ge $timeout ]] && { echo "ERROR: Timeout waiting for job"; exit 1; }
          done
          echo "Job started"
      
      - name: Get Hostname
        id: get_hostname
        early-cancel: any-job-failed
        run: |
          set -x
          cd ${{ needs.setup.outputs.rundir }}
          
          # Wait for HOSTNAME file (written by start_service.sh via script_submitter)
          timeout=120; elapsed=0
          while [ ! -f HOSTNAME ]; do
            sleep 2; ((elapsed+=2))
            echo "Waiting for HOSTNAME file... (${elapsed}s/${timeout}s)"
            [[ $elapsed -ge $timeout ]] && break
          done
          
          if [ -f HOSTNAME ]; then
            target_hostname=$(cat HOSTNAME 2>/dev/null | head -1)
          else
            # Fallback: if no HOSTNAME file, try getting from scheduler or localhost
            if [[ "${{ inputs.scheduler.enabled }}" == "true" ]]; then
              source jobid 2>/dev/null || true
              if [[ -n "$jobid" ]]; then
                # Try SLURM first
                target_hostname=$(squeue -j "${jobid}" --noheader --format="%N" 2>/dev/null || true)
              fi
            fi
            # Last resort: use login node
            [[ -z "${target_hostname}" ]] && target_hostname=$(hostname)
          fi
          
          [[ -z "${target_hostname}" ]] && { echo "$(date) Failed to get target hostname"; exit 1; }
          echo "target_hostname=${target_hostname}" | tee -a $OUTPUTS
      
      - name: Get Session Port
        id: get_port
        early-cancel: any-job-failed
        run: |
          set -euo pipefail
          set -x
          
          TIMEOUT=600
          RETRY_INTERVAL=3
          cd ${{ needs.setup.outputs.rundir }}
          
          attempt=1; elapsed=0
          while true; do
            echo "$(date) Attempt $attempt: Checking for SESSION_PORT..."
            
            if [ -f SESSION_PORT ]; then
              echo "$(date) Success: SESSION_PORT found!"
              PORT=$(cat SESSION_PORT)
              echo "SESSION_PORT=$PORT" | tee -a "$OUTPUTS"
              exit 0
            elif [ -f job.ended ]; then
              echo "$(date) Job ended without SESSION_PORT"
              exit 1
            else
              echo "$(date) Waiting... (${elapsed}s/${TIMEOUT}s)"
              sleep "$RETRY_INTERVAL"
              ((attempt++)); ((elapsed+=RETRY_INTERVAL))
              [[ $elapsed -ge $TIMEOUT ]] && { echo "$(date) Timeout"; exit 1; }
            fi
          done
      
      - name: Wait for Server
        id: wait_server
        early-cancel: any-job-failed
        run: |
          TIMEOUT=300
          RETRY_INTERVAL=3
          remote_host="${{ needs.create_session.steps.get_hostname.outputs.target_hostname }}"
          remote_port="${{ needs.create_session.steps.get_port.outputs.SESSION_PORT }}"
          
          cd ${{ needs.setup.outputs.rundir }}
          
          attempt=1; elapsed=0
          while true; do
            echo "$(date) Attempt $attempt: Checking ${remote_host}:${remote_port}..."
            
            if curl --silent --connect-timeout 5 "http://${remote_host}:${remote_port}" >/dev/null 2>&1; then
              echo "$(date) Success: Server is listening!"
              exit 0
            elif [ -f job.ended ]; then
              echo "$(date) Job completed"
              exit 0
            else
              echo "$(date) Waiting... (${elapsed}s/${TIMEOUT}s)"
              sleep "$RETRY_INTERVAL"
              ((attempt++)); ((elapsed+=RETRY_INTERVAL))
              [[ $elapsed -ge $TIMEOUT ]] && { echo "$(date) Timeout"; exit 1; }
            fi
          done
      
      - name: Update Session
        id: update_session
        uses: parallelworks/update-session
        with:
          remotePort: '${{ needs.create_session.steps.get_port.outputs.SESSION_PORT }}'
          target: '${{ inputs.resource.id }}'
          name: '${{ sessions.session }}'
          remoteHost: '${{ needs.create_session.steps.get_hostname.outputs.target_hostname }}'
          localPort: '${{ inputs.advanced_settings.localport }}'

# ==============================================================================
# INPUT DEFINITIONS
# ==============================================================================
'on':
  execute:
    inputs:
      # ========================================================================
      # Resource Selection
      # ========================================================================
      resource:
        type: compute-clusters
        label: Compute Cluster
        autoselect: true
        include-workspace: false
        tooltip: Resource to run the service
      
      # ========================================================================
      # Scheduler Configuration (for SLURM/PBS job submission)
      # ========================================================================
      scheduler:
        type: group
        label: Scheduler Options
        tooltip: Configure job scheduler (SLURM/PBS) for HPC execution
        items:
          enabled:
            type: boolean
            label: Submit to Job Scheduler
            tooltip: Enable to submit job via SLURM or PBS scheduler (detected automatically from resource). Disable for direct SSH execution.
            default: false
            hidden: ${{ inputs.resource.schedulerType == '' }}
          
          slurm:
            type: group
            label: SLURM Configuration
            hidden: ${{ inputs.scheduler.enabled != true || inputs.resource.schedulerType != 'slurm' }}
            collapsed: true
            items:
              is_disabled:
                type: boolean
                default: ${{ inputs.resource.schedulerType != 'slurm' }}
                hidden: true
              
              account:
                label: Account
                type: slurm-accounts
                resource: ${{ inputs.resource }}
                tooltip: SLURM account for job submission (--account)
                optional: true
              
              partition:
                type: slurm-partitions
                label: Partition
                optional: true
                resource: ${{ inputs.resource }}
                tooltip: Partition for job submission (--partition)
              
              qos:
                label: Quality of Service (QoS)
                type: slurm-qos
                resource: ${{ inputs.resource }}
                tooltip: SLURM QoS setting (--qos)
                optional: true
              
              cpus_per_task:
                type: number
                label: CPUs per Task
                min: 1
                max: 256
                default: 4
                tooltip: Number of CPUs for the job (--cpus-per-task)
              
              time:
                label: Walltime
                type: string
                default: 04:00:00
                tooltip: Maximum job duration, e.g., 04:00:00 (--time)
              
              gres:
                label: GPUs (gres)
                type: string
                default: "gpu:1"
                placeholder: "gpu:1"
                tooltip: GPU resource specification, e.g., gpu:1, gpu:a100:2 (--gres)
              
              scheduler_directives:
                type: editor
                label: Additional Directives
                optional: true
                tooltip: Additional SLURM directives (include #SBATCH prefix)
          
          pbs:
            type: group
            label: PBS Configuration
            hidden: ${{ inputs.scheduler.enabled != true || inputs.resource.schedulerType != 'pbs' }}
            collapsed: true
            items:
              is_disabled:
                type: boolean
                default: ${{ inputs.resource.schedulerType != 'pbs' }}
                hidden: true
              
              account:
                label: Account
                type: string
                optional: true
                tooltip: PBS account for job submission (-A)
              
              queue:
                label: Queue
                type: string
                optional: true
                tooltip: PBS queue name (-q)
              
              walltime:
                label: Walltime
                type: string
                default: 04:00:00
                tooltip: Maximum job duration, e.g., 04:00:00 (-l walltime=)
              
              select:
                label: Resource Selection
                type: string
                default: "1:ncpus=8:ngpus=1"
                placeholder: "1:ncpus=8:ngpus=1"
                tooltip: PBS resource selection string, e.g., 1:ncpus=8:ngpus=1 (-l select=)
              
              scheduler_directives:
                label: Additional Directives
                type: editor
                tooltip: Additional PBS directives (include #PBS prefix)
                optional: true
      
      runmode:
        label: Container Runtime
        type: dropdown
        default: singularity
        tooltip: Apptainer/Singularity is recommended for HPC environments
        options:
          - value: singularity
            label: Apptainer
          - value: docker
            label: Docker
      
      runtype:
        label: Deployment Type
        type: dropdown
        default: vllm
        tooltip: Deploy vLLM only or full vLLM+RAG stack
        options:
          - value: vllm
            label: vLLM Only
          - value: all
            label: vLLM + RAG (Full Stack)

      # ========================================================================
      # Model Configuration
      # ========================================================================
      model:
        type: group
        label: Model Configuration
        items:
          source:
            type: dropdown
            label: Model Source
            default: huggingface
            tooltip: Use a pre-downloaded model (local) or clone from HuggingFace using git-lfs
            options:
              - value: local
                label: "üìÅ Local Path (pre-staged weights)"
              - value: huggingface
                label: "ü§ó HuggingFace Clone (git-lfs)"
          
          local_path:
            type: string
            label: Model Path
            placeholder: /path/to/model/weights
            default: /models/Llama-3_3-Nemotron-Super-49B-v1_5
            tooltip: Full path to directory containing model weights
            hidden: ${{ inputs.model.source != 'local' }}
            ignore: ${{ inputs.model.source != 'local' }}
          
          hf_model_id:
            type: string
            label: HuggingFace Model ID
            placeholder: Qwen/Qwen2.5-Coder-7B-Instruct
            default: Qwen/Qwen2.5-Coder-7B-Instruct
            tooltip: Model ID to clone from HuggingFace. Default is Qwen2.5-Coder-7B optimized for code assist on single GPU.
            hidden: ${{ inputs.model.source != 'huggingface' }}
            ignore: ${{ inputs.model.source != 'huggingface' }}
          
          hf_token:
            label: HuggingFace Token
            optional: true
            default: ${{ org.HF_TOKEN }}
            type: password
            tooltip: Required for gated models (Llama, etc.)
            hidden: ${{ inputs.model.source != 'huggingface' }}
            ignore: ${{ inputs.model.source != 'huggingface' }}
          
          cache_dir:
            type: string
            label: Model Cache Directory
            default: ~/pw/models
            tooltip: Directory to clone model into. Model is cloned once and reused for subsequent runs. Ensure sufficient disk space.
            hidden: ${{ inputs.model.source != 'huggingface' }}
            ignore: ${{ inputs.model.source != 'huggingface' }}

      # ========================================================================
      # vLLM Configuration
      # ========================================================================
      vllm:
        type: group
        label: vLLM Settings
        collapsed: true
        items:
          num_gpus:
            type: dropdown
            label: Number of GPUs
            default: "1"
            tooltip: Number of GPUs for tensor parallelism (must match available GPUs)
            options:
              - value: "1"
                label: "1 GPU"
              - value: "2"
                label: "2 GPUs"
              - value: "4"
                label: "4 GPUs"
              - value: "8"
                label: "8 GPUs"
          
          gpu_memory:
            type: dropdown
            label: GPU Memory Utilization
            default: "0.90"
            tooltip: Fraction of GPU memory to use (lower if OOM errors occur)
            options:
              - value: "0.95"
                label: "95% (Maximum)"
              - value: "0.90"
                label: "90%"
              - value: "0.85"
                label: "85% (Recommended)"
              - value: "0.80"
                label: "80%"
              - value: "0.70"
                label: "70% (Conservative)"
          
          max_model_len:
            type: dropdown
            label: Max Context Length
            default: "auto"
            tooltip: Maximum sequence length. Lower values reduce memory usage.
            options:
              - value: "auto"
                label: "Auto (use model default)"
              - value: "4096"
                label: "4K tokens"
              - value: "8192"
                label: "8K tokens"
              - value: "16384"
                label: "16K tokens"
              - value: "32768"
                label: "32K tokens"
              - value: "65536"
                label: "64K tokens"
              - value: "131072"
                label: "128K tokens"
          
          dtype:
            type: dropdown
            label: Data Type
            default: bfloat16
            tooltip: Model precision. Select 'Custom' to specify via extra arguments.
            options:
              - value: bfloat16
                label: "bfloat16 (Recommended for A100/H100)"
              - value: float16
                label: "float16 (Better compatibility)"
              - value: auto
                label: "Auto (model default)"
              - value: custom
                label: "Custom (specify in extra args)"
          
          extra_args:
            type: string
            label: Additional Arguments
            default: "--trust_remote_code --async-scheduling"
            placeholder: "--trust_remote_code"
            tooltip: Additional vLLM arguments. If dtype is 'Custom', include --dtype here.
            optional: true

      # ========================================================================
      # RAG Configuration
      # ========================================================================
      rag:
        type: group
        label: RAG Settings
        hidden: ${{ inputs.runtype != 'all' }}
        collapsed: true
        items:
          docsdir:
            label: Documents Directory
            optional: true
            default: ./docs
            type: string
            tooltip: Directory containing documents to index for RAG
          
          systemprompt:
            type: string
            label: System Prompt
            textarea: true
            optional: true
            default: You are a careful assistant. Use ONLY the provided context blocks to answer. Each block is numbered [1], [2], ‚Ä¶ and includes source metadata. When you use information from a block, you MUST cite it inline with [n]. At the end of your response, include a 'References' section with one reference per line formatted as [n] file_path (chunk index). Do not invent citations or sources. If the context does not contain the answer, say so briefly.

      # ========================================================================
      # Container Configuration
      # ========================================================================
      container:
        type: group
        label: Container Options
        hidden: ${{ inputs.runmode != 'singularity' }}
        collapsed: true
        items:
          source:
            type: dropdown
            label: Container Source
            default: path
            tooltip: How to obtain Apptainer/Singularity containers
            options:
              - value: path
                label: "üìÇ Use existing path (default)"
              - value: pull
                label: "üì• Pull from bucket"
              - value: build
                label: "üî® Build from source (requires sudo/fakeroot)"
          
          bucket:
            label: Container Bucket
            type: string
            default: pw://mshaxted/codeassist
            tooltip: Bucket containing vllm.sif and rag.sif containers
            hidden: ${{ inputs.container.source != 'pull' }}
            ignore: ${{ inputs.container.source != 'pull' }}
          
          vllm_path:
            label: vLLM Container Path
            type: string
            default: ~/pw/singularity/vllm.sif
            placeholder: ~/pw/singularity/vllm.sif
            tooltip: Path to vllm.sif container (use existing or build destination)
            hidden: ${{ inputs.container.source == 'pull' }}
            ignore: ${{ inputs.container.source == 'pull' }}
          
          rag_path:
            label: RAG Container Path
            type: string
            default: ~/pw/singularity/rag.sif
            placeholder: ~/pw/singularity/rag.sif
            tooltip: Path to rag.sif container (use existing or build destination, only needed for vLLM+RAG mode)
            hidden: ${{ inputs.container.source == 'pull' || inputs.runtype != 'all' }}
            ignore: ${{ inputs.container.source == 'pull' || inputs.runtype != 'all' }}

      # ========================================================================
      # Advanced Settings
      # ========================================================================
      advanced_settings:
        type: group
        label: Advanced Settings
        collapsed: true
        items:
          localport:
            label: Local Port
            default: '5555'
            tooltip: Port in your workspace to access the service
            type: string
          
          apikey:
            label: vLLM API Key
            optional: true
            tooltip: Optional API key for vLLM server. Can be any value (e.g., 'dummy') - Cline and other IDEs require a value but do not validate it.
            type: password
          
          rundir:
            label: Run Directory
            default: ~/pw/activate-rag-vllm
            type: string
            tooltip: Directory where the service will be deployed
          
          repository:
            type: string
            label: Repository
            default: https://github.com/parallelworks/activate-rag-vllm.git
          
          repository_branch:
            type: string
            label: Repository Branch
            default: refactor
          
          tiktoken_encodings:
            label: Download Tiktoken Encodings
            tooltip: Download tiktoken encodings for offline use (required for some models)
            type: boolean
            default: false
          
          vllm_attention_backend:
            type: dropdown
            label: vLLM Attention Backend
            default: FLASH_ATTN
            tooltip: Attention implementation used by vLLM
            options:
              - value: FLASH_ATTN
              - value: TRITON_ATTN
              - value: FLASHINFER
              - value: FLASHINFER_MLA
              - value: TRITON_MLA
              - value: CUTLASS_MLA
              - value: FLASHMLA
              - value: FLASHMLA_SPARSE
              - value: FLASH_ATTN_MLA
              - value: TORCH_SDPA
              - value: ROCM_ATTN
              - value: ROCM_AITER_MLA
              - value: ROCM_AITER_TRITON_MLA
              - value: ROCM_AITER_FA
              - value: PALLAS
              - value: IPEX
              - value: CPU_ATTN
              - value: NO_ATTENTION
              - value: CUSTOM
