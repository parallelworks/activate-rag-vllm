
networks:
  ragnet: {}

services:

  vllm:
    image: vllm/vllm-openai:latest
    gpus: all
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      VLLM_SERVER_PORT: ${VLLM_SERVER_PORT}
      HF_TOKEN: ${HF_TOKEN}
      HF_HOME: /root/.cache/huggingface
      VLLM_API_KEY: ${VLLM_API_KEY}
    volumes:
      - ./cache:/root/.cache
    command: >
      --model ${MODEL_NAME}
      --host 0.0.0.0
      --port ${VLLM_SERVER_PORT}
      ${VLLM_EXTRA_ARGS}
    ports:
      - "${VLLM_SERVER_PORT}:${VLLM_SERVER_PORT}"
    networks: [ragnet]
    healthcheck:
      test: ["CMD-SHELL", "curl -sSf http://localhost:${VLLM_SERVER_PORT}/v1/models >/dev/null || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 100
      start_period: 30s

  rag:
    build:
      context: .
      dockerfile: Dockerfile.rag
    image: parallelworks/activate-rag-vllm:latest
    environment:
      RAG_URL: ${RAG_URL}
      VLLM_URL: ${VLLM_URL}
      RAG_PORT: ${RAG_PORT}
      VLLM_SERVER_PORT: ${VLLM_SERVER_PORT}
      CHROMA_PORT: ${CHROMA_PORT}
      CHROMA_HOST: ${CHROMA_HOST}      
      EMBEDDING_MODEL: ${EMBEDDING_MODEL}
      EMBEDDING_DEVICE: cpu
      MODEL_NAME: ${MODEL_NAME}
      MAX_CONTEXT: ${MAX_CONTEXT}
      MAX_TOKENS: ${MAX_TOKENS}
      TEMPERATURE: ${TEMPERATURE}
      TOP_K: ${TOP_K}
      STREAM_READ_TIMEOUT: none
      STREAM_BRIDGE: ${STREAM_BRIDGE:-1}
      DEBUG_SSE: ${DEBUG_SSE:-0}
      IGNORE_CLIENT_STOP: ${IGNORE_CLIENT_STOP:-1}
      HF_TOKEN: ${HF_TOKEN}
      HF_HOME: /root/.cache/huggingface
    volumes:
      - ./logs:/logs
      - ./cache:/root/.cache
      - ${DOCS_DIR}:/docs
    ports:
      - "${PROXY_PORT}:8081"
    networks: [ragnet]
    depends_on:
      vllm:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -sSf http://localhost:8081/health >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 10s
