# ---- General ports ----
VLLM_SERVER_PORT=8000
RAG_PORT=8080
PROXY_PORT=8081
CHROMA_PORT=8001
CHROMA_HOST=127.0.0.1

RUN_OPENWEBUI=0
OPENWEBUI_PORT=3000

RAG_URL="http://127.0.0.1:$RAG_PORT"
VLLM_URL="http://vllm:$VLLM_SERVER_PORT/v1"

# ---- Model & vLLM ----
MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
DOCS_DIR=./docs
# Recommended on T4/V100 and for mistral tokenizer
VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1
VLLM_USE_FLASHINFER_SAMPLER=0

VLLM_EXTRA_ARGS="VLLM_EXTRA_ARGS="__VLLM_EXTRA_ARGS__"

TOOL_PARSER="llama3_json"
CHAT_TEMPLATE=${CHAT_TEMPLATE:-example/tool_chat_template_llama3.1_json.jinja}
# --enable-auto-tool-choice --tool-call-parser $TOOL_PARSER --chat-template $CHAT_TEMPLATE

# If model is gated on HF:
# HF_TOKEN=hf_xx
#TRANSFORMERS_OFFLINE=1

# ---- RAG settings ----
MAX_CONTEXT=8192
MAX_TOKENS=256
TEMPERATURE=0.2
TOP_K=4
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DEVICE=cpu
CHROMA_AUTO_RESET=0

# ---------- Misc ----------
STREAM_BRIDGE=1
DEBUG_SSE=0
IGNORE_CLIENT_STOP=1
