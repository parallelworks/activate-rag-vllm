# ---- General ports ----
VLLM_SERVER_PORT=8000
RAG_PORT=8080
PROXY_PORT=8081
CHROMA_PORT=8001
CHROMA_HOST=127.0.0.1

RUN_OPENWEBUI=0
OPENWEBUI_PORT=3000

RAG_URL="http://127.0.0.1:$RAG_PORT"
VLLM_URL="http://vllm:$VLLM_SERVER_PORT/v1"

# ---- Model & vLLM ----
MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
DOCS_DIR=./docs
# Recommended on T4/V100 and for mistral tokenizer
VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1
VLLM_EXTRA_ARGS="--dtype float16 --max-model-len 8192 --gpu-memory-utilization=0.85"
# If model is gated on HF:
# HF_TOKEN=hf_xx
#TRANSFORMERS_OFFLINE=1

# ---- RAG settings ----
MAX_CONTEXT=8192
MAX_TOKENS=256
TEMPERATURE=0.2
TOP_K=4
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DEVICE=cpu
CHROMA_AUTO_RESET=0

# ---------- Misc ----------
STREAM_BRIDGE=1
DEBUG_SSE=0
IGNORE_CLIENT_STOP=1