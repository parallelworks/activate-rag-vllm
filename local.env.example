# ==============================================================================
# Local Development Configuration for activate-rag-vllm
# ==============================================================================
# Copy this file and customize for your environment:
#   cp local.env.example my-config.env
#   ./run_local.sh --config my-config.env
# ==============================================================================

# ---- Runtime Selection ----
# Options: auto, singularity, docker
# 'auto' will prefer Singularity if available
RUNMODE=auto

# ---- Deployment Type ----
# Options: all (vLLM + RAG), vllm (vLLM only)
RUNTYPE=all

# ---- Model Configuration ----
# Source: 'local' for pre-downloaded models, 'huggingface' to download
MODEL_SOURCE=local

# For local models: full path to model weights directory
MODEL_PATH=/models/Llama-3.1-8B-Instruct

# For HuggingFace models: model ID and optional token
#MODEL_SOURCE=huggingface
#HF_MODEL_ID=meta-llama/Llama-3.1-8B-Instruct
#HF_TOKEN=hf_xxxxxxxxxxxxx

# Where to cache downloaded models
MODEL_CACHE_DIR=~/.cache/activate-models

# ---- Service Ports ----
# Set to 0 for auto-assignment, or specify fixed ports
VLLM_SERVER_PORT=0
PROXY_PORT=0
RAG_PORT=0
CHROMA_PORT=0

# ---- RAG Configuration ----
# Directory containing documents to index
DOCS_DIR=./docs

# ---- vLLM Configuration ----
# Extra arguments passed to vLLM server
VLLM_EXTRA_ARGS="--dtype bfloat16 --trust_remote_code"

# For multi-GPU setups, add tensor parallelism:
#VLLM_EXTRA_ARGS="--dtype bfloat16 --trust_remote_code --tensor-parallel-size 4"

# ---- Security ----
# API key for vLLM server (optional)
#API_KEY=your-api-key-here

# ---- Build Options ----
# Set to true to build containers from source instead of using pre-built
BUILD=false

# ---- Debug ----
# Enable debug output
#DEBUG=1
