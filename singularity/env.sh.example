# ---- General ports ----
export VLLM_SERVER_PORT=8000
export RAG_PORT=8080
export PROXY_PORT=8081 # primary connection port
export CHROMA_PORT=8001
export CHROMA_HOST=127.0.0.1

export RUN_OPENWEBUI=0
export OPENWEBUI_PORT=3000

export RAG_URL="http://127.0.0.1:$RAG_PORT"
export VLLM_URL="http://127.0.0.1:$VLLM_SERVER_PORT/v1"

# ---- Model & vLLM ----
export PYTHONNOUSERSITE=1
export MODEL_NAME="meta-llama/Llama-3.1-8B-Instruct"
export MAX_CONTEXT=8192

# If model is gated on HF:
export HF_TOKEN="hf_xxx"
export HF_HOME="/root/.cache/huggingface"
#export TRANSFORMERS_OFFLINE=1

# Recommended on T4/V100 and for mistral tokenizer
export DOCS_DIR=./docs
export VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1
export VLLM_EXTRA_ARGS="__VLLM_EXTRA_ARGS__"
export TRITON_CC=gcc
export CC=/usr/bin/gcc
export CXX=/usr/bin/g++


# Other VLLM tuning settings
export VLLM_LOGGING_LEVEL=INFO

# ---- RAG settings ----
export MAX_TOKENS=256
export TEMPERATURE=0.2
export TOP_K=4
export EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
export EMBEDDING_DEVICE=cpu

# ---- Chroma & Indexer settings ----
export CHROMA_AUTO_RESET=0
export CHROMA_HOST=127.0.0.1
export CHROMA_PORT=8001
export ANONYMIZED_TELEMETRY=False

export INDEXER_RESCAN_SECONDS=10
export INDEXER_LOGLEVEL=INFO

# ---------- Misc ----------
export STREAM_BRIDGE=1
export DEBUG_SSE=0
export IGNORE_CLIENT_STOP=1
