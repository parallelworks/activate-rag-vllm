# ==============================================================================
# ACTIVATE RAG-vLLM Environment Configuration
# ==============================================================================
# This file is sourced by Singularity containers at runtime.
# Copy to env.sh and modify as needed, or let start_service.sh generate it.
# ==============================================================================

# ---- Service Ports ----
export VLLM_SERVER_PORT=8000
export RAG_PORT=8080
export PROXY_PORT=8081     # Primary connection port (OpenAI-compatible API)
export CHROMA_PORT=8001
export CHROMA_HOST=127.0.0.1

# ---- Optional Services ----
export RUN_OPENWEBUI=0
export OPENWEBUI_PORT=3000

# ---- Internal URLs (auto-configured) ----
export RAG_URL="http://127.0.0.1:$RAG_PORT"
export VLLM_URL="http://127.0.0.1:$VLLM_SERVER_PORT/v1"

# ==============================================================================
# Model Configuration
# ==============================================================================
export MODEL_NAME="meta-llama/Llama-3.1-8B-Instruct"
export MAX_CONTEXT=8192

# HuggingFace settings
export HF_TOKEN=""
export HF_HOME="/root/.cache/huggingface"

# Offline mode (recommended for HPC)
# Set to 1 when using pre-downloaded models
#export TRANSFORMERS_OFFLINE=1

# ==============================================================================
# vLLM Configuration
# ==============================================================================
export PYTHONNOUSERSITE=1
export DOCS_DIR=./docs

# Attention backend - options include:
#   FLASH_ATTN, TRITON_ATTN, FLASHINFER, TORCH_SDPA, etc.
export VLLM_ATTENTION_BACKEND=FLASH_ATTN

# Extra vLLM arguments (will be appended to server command)
export VLLM_EXTRA_ARGS="__VLLM_EXTRA_ARGS__"

# Compiler settings for JIT compilation
export TRITON_CC=gcc
export CC=/usr/bin/gcc
export CXX=/usr/bin/g++

# ==============================================================================
# Cache and Temp Directories
# ==============================================================================
export TMPDIR=${PWD}/tmp
export CUDA_CACHE_PATH=${TMPDIR}/cuda_cache
export TORCH_EXTENSIONS_DIR=${TMPDIR}/torch_extensions
export FLASHINFER_JIT_DIR=${TMPDIR}/flashinfer_jit

# Tiktoken encodings (for offline tokenizer support)
#export TIKTOKEN_CACHE_DIR=${PWD}/cache/tiktoken_encodings

# ==============================================================================
# vLLM Tuning
# ==============================================================================
export VLLM_LOGGING_LEVEL=INFO

# ==============================================================================
# RAG Settings
# ==============================================================================
export MAX_TOKENS=256
export TEMPERATURE=0.2
export TOP_K=4
export EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
export EMBEDDING_DEVICE=cpu

# ==============================================================================
# ChromaDB & Indexer Settings
# ==============================================================================
export CHROMA_AUTO_RESET=0
export CHROMA_HOST=127.0.0.1
export CHROMA_PORT=8001
export ANONYMIZED_TELEMETRY=False

export INDEXER_RESCAN_SECONDS=10
export INDEXER_LOGLEVEL=INFO

# ---------- Misc ----------
export STREAM_BRIDGE=1
export DEBUG_SSE=0
export IGNORE_CLIENT_STOP=1
