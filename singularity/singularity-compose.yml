version: "2.0"

instances:
  
  vllm:
    build:
      context: .
      recipe: Singularity.vllm
    image: vllm.sif
    network:
      enable: false
    # Pass GPU flag at instance start so the runtime has CUDA inside
    start:
      options:
        - nv
    run:
      args:
        - >
          nohup python3 -m vllm.entrypoints.openai.api_server --model "${MODEL_NAME}" --tokenizer "${MODEL_NAME}" --host 0.0.0.0 --port ${VLLM_SERVER_PORT} ${VLLM_EXTRA_ARGS} > /logs/vllm.out 2>&1 &
    volumes:
      - ./logs:/logs
      - ./cache:/root/.cache
      - ./env.sh:/.singularity.d/env/env.sh
      - __MODEL_NAME__:__MODEL_NAME__

  rag:
    build:
      context: .
      recipe: Singularity.rag
    image: rag.sif
    network:
      enable: false
    volumes:
      - ./logs:/logs
      - ./cache:/root/.cache
      - ./docs:/docs
      - ./env.sh:/.singularity.d/env/env.sh
    run:
      args: {}
    depends_on:
      - vllm